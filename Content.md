# Group O COMP7630 Project
# 1.Dataset

This dataset contains around 240k news headlines from 2012 to 2025 from HuffPost. Based on kaggle dataset (2012-2022) and new post scrapped directly from Huffpost.com (2022 - 2025). It's one of the biggest news datasets and can serve as a benchmark for a variety of tasks.

Each record in the dataset consists of the following attributes:

category: category in which the article was published.

headline: the headline of the news article.

authors: list of authors who contributed to the article.

link: link to the original news article.

short_description: Abstract of the news article.

date: publication date of the article.

example:

```js
{
  "link": "https://www.huffpost.com/entry/covid-boosters-uptake-us_n_632d719ee4b087fae6feaac9",
  "headline": "Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters",
  "category": "U.S. NEWS",
  "short_description": "Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.",
  "authors": "Carla K. Johnson, AP",
  "date": "2022-09-23"
}
```
 # Environment Setup if running on Google Colab


```js
from google.colab import drive
import pandas as pd


drive.mount('/content/drive')
json_path = '/content/drive/MyDrive/data/News_Category_Dataset_v3.json'
df = pd.read_json(json_path, lines=True)
```

# Environment Setup local

```js
import pandas as pd


json_path = "./News_Category_Dataset_v3.json"

df = pd.read_json(json_path, lines=True)
```

```js
# Verify the data
df.head()
```

# 2.Dataset Analysis and Data Preprocessing.

Data Preprocessing

Sentence segmentation and tokenization
Remove URLs and email addresses
Stop word removal
Filter out short sentences
Data Analysis (Analyze before and after preprocessing)

Analyze text length distribution
Analyze class distribution
Calculate keywords for each category
Generate word clouds for visualization

```js
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from collections import Counter
import nltk

# Init
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

extra_stopwords = ['said', 'say', 'would', 'could', 'also', 'one', 'two', 'make', 'may',
                   'u', 'time', 'new']
stop_words.update(extra_stopwords)

print(stop_words)

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    words = word_tokenize(text)
    words = [word for word in words if word not in stop_words]
    words = [lemmatizer.lemmatize(word) for word in words]
    return ' '.join(words)

```

```js
# 1. Sentence segmentation and tokenization

df.dropna(subset=['headline', 'short_description'], inplace=True)

df['text'] = df['headline'] + ' ' + df['short_description'].fillna('')

df['cleaned_text'] = df['text'].apply(preprocess_text)
```


```js
# 2. display the distribution of text length and category

import matplotlib.pyplot as plt
import seaborn as sns

df['text_length'] = df['text'].str.len()
fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 24))

# Plot 1: Distribution of text length
sns.histplot(data=df, x='text_length', bins=30, ax=ax1)
ax1.set_title('Distribution of Text Length')
ax1.set_xlabel('Text Length (characters)')
ax1.set_ylabel('Count')

# Plot 2: Box plot of text length by category
sns.boxplot(data=df, x='category', y='text_length', ax=ax2)
ax2.set_title('Text Length Distribution by Category')
ax2.set_xlabel('Category')
ax2.set_ylabel('Text Length (characters)')
plt.xticks(rotation=45, ha='right')  # Fixed: using plt.xticks instead of ax2.xticks


# Plot 3:
category_counts = df['category'].value_counts()
sns.barplot(x=category_counts.index, y=category_counts.values, ax=ax3)
ax3.set_title('Distribution of News Categories')
ax3.set_xlabel('Category')
ax3.set_ylabel('Count')
plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')

# Plot 4: word cloud
from wordcloud import WordCloud
text = ' '.join(df['cleaned_text'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
ax4.imshow(wordcloud)
ax4.set_title('Word Cloud of News Content')
ax4.axis('off')

plt.tight_layout()
plt.show()
```

```js
# filter out low frequency words and short words

all_words = ' '.join(df['cleaned_text']).split()
word_counts = Counter(all_words)
min_freq = 5
min_length = 3
df['filtered_text'] = df['cleaned_text'].apply(
    lambda x: ' '.join([word for word in x.split()
    if word_counts[word] >= min_freq and len(word) >= min_length])
)

top_categories = df['category'].value_counts().head(15).index
df = df[df['category'].isin(top_categories)]

# å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆç¤ºä¾‹ï¼šè¿‡é‡‡æ ·ï¼‰
from imblearn.over_sampling import RandomOverSampler
X = df['filtered_text']
y = df['category']
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X.to_frame(), y)
df_balanced = pd.DataFrame({'text': X_resampled['filtered_text'], 'category': y_resampled})

df_balanced.to_csv('preprocessed_news.csv', index=False)
```

```js
# display distribution after preprocessing

df_balanced['text_length'] = df_balanced['text'].str.len()
fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 18))

# Plot 1: Distribution of category
sns.histplot(data=df_balanced, x='category', bins=30, ax=ax1)
ax1.set_title('Distribution of Category')
ax1.set_xlabel('Category')
ax1.set_ylabel('Count')
plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')

# Plot 2: Top 20 words bar chart
all_words = ' '.join(df_balanced['text']).split()
word_counts = Counter(all_words)
top_words = word_counts.most_common(20)
words, counts = zip(*top_words)

sns.barplot(x=list(counts), y=list(words), palette='viridis', ax=ax2)
ax2.set_title('Top 20 Most Frequent Words')
ax2.set_xlabel('Count')
ax2.set_ylabel('Words')


# Plot 3: Overall word cloud visualization
text = ' '.join(df_balanced['text'])
wordcloud = WordCloud(width=800, height=400,
                     background_color='white',
                     max_words=100,
                     colormap='viridis').generate(text)

ax3.imshow(wordcloud)
ax3.set_title('Overall Word Cloud')
ax3.axis('off')

plt.tight_layout()
plt.show()
```

```js
# Add dropdown to show top 15 frequent words by category
from ipywidgets import interact, Dropdown

def show_category_top_words(category):
    plt.figure(figsize=(10, 6))

    # Get text for selected category
    category_text = ' '.join(df_balanced[df_balanced['category'] == category]['text'])

    # Count word frequencies
    words = category_text.split()
    word_counts = Counter(words)

    # Get top 15 words
    top_words = word_counts.most_common(15)
    words, counts = zip(*top_words)

    # Create bar chart
    sns.barplot(x=list(counts), y=list(words), palette='viridis')
    plt.title(f'Top 15 Most Frequent Words in Category: {category}')
    plt.xlabel('Count')
    plt.ylabel('Words')
    plt.tight_layout()
    plt.show()

# This will create an interactive dropdown in the notebook
interact(show_category_top_words,
         category=Dropdown(options=sorted(df_balanced['category'].unique()),
                          description='Category:'))
```


# Topic Modeling

Topic Modeling
LDA

```js
# init

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation
from bertopic import BERTopic
import pyLDAvis
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Topic number
n_topics = 10
```


```js
# 2. LDA (Latent Dirichlet Allocation)
print("Training LDA model...")

# åˆ›å»ºè¯é¢‘çŸ©é˜µ
count_vectorizer = CountVectorizer(max_features=5000)
count_matrix = count_vectorizer.fit_transform(df['cleaned_text'])

# è®­ç»ƒLDAæ¨¡å‹
lda_model = LatentDirichletAllocation(
    n_components=n_topics,
    random_state=42,
    max_iter=50
)
lda_output = lda_model.fit_transform(count_matrix)

# è·å–ä¸»é¢˜è¯
feature_names = count_vectorizer.get_feature_names_out()
lda_topics = {}
for topic_idx, topic in enumerate(lda_model.components_):
    top_words = [feature_names[i] for i in topic.argsort()[:-10:-1]]
    lda_topics[f"Topic {topic_idx+1}"] = top_words
    print(f"Topic {topic_idx+1}: {', '.join(top_words)}")
```

```js
def visualize_lda_results(lda_model, count_vectorizer, lda_output, n_words=10):
    # 1. ä¸»é¢˜-è¯è¯­åˆ†å¸ƒå¯è§†åŒ–
    feature_names = count_vectorizer.get_feature_names_out()

    # åˆ›å»ºä¸€ä¸ªå¤§å›¾ï¼ŒåŒ…å«æ‰€æœ‰ä¸»é¢˜
    n_topics = len(lda_model.components_)
    n_rows = (n_topics + 4) // 5  # æ¯è¡Œ5ä¸ªä¸»é¢˜
    fig = plt.figure(figsize=(20, 4*n_rows))

    for i, topic in enumerate(lda_model.components_):
        plt.subplot(n_rows, 5, i + 1)
        top_words_idx = topic.argsort()[:-n_words-1:-1]
        top_words = [feature_names[idx] for idx in top_words_idx]
        top_weights = topic[top_words_idx]

        plt.barh(top_words, top_weights)
        plt.title(f'Topic {i+1}')
        plt.xlabel('Weight')

    plt.tight_layout()
    plt.show()

    # 2. ä¸»é¢˜åˆ†å¸ƒçƒ­åŠ›å›¾
    plt.figure(figsize=(12, 8))
    topic_term_matrix = pd.DataFrame(
        lda_model.components_,
        columns=feature_names,
        index=[f'Topic {i+1}' for i in range(n_topics)]
    )
    sns.heatmap(topic_term_matrix.iloc[:, :30], cmap='YlOrRd')
    plt.title('Topic-Term Heatmap')
    plt.xlabel('Terms')
    plt.ylabel('Topics')
    plt.show()

    # 3. æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ
    doc_topics = pd.DataFrame(
        lda_output,
        columns=[f'Topic {i+1}' for i in range(n_topics)]
    )

    plt.figure(figsize=(10, 6))
    doc_topics.mean().plot(kind='bar')
    plt.title('Average Topic Distribution Across Documents')
    plt.xlabel('Topics')
    plt.ylabel('Average Weight')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    # 4. ä¸»é¢˜ç›¸å…³æ€§
    plt.figure(figsize=(10, 8))
    topic_corr = np.corrcoef(lda_output.T)
    sns.heatmap(
        topic_corr,
        annot=True,
        cmap='coolwarm',
        xticklabels=[f'T{i+1}' for i in range(n_topics)],
        yticklabels=[f'T{i+1}' for i in range(n_topics)]
    )
    plt.title('Topic Correlations')
    plt.show()

    # 5. æ‰“å°æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯
    print("\nTop words in each topic:")
    for i, topic in enumerate(lda_model.components_):
        top_words_idx = topic.argsort()[:-n_words-1:-1]
        top_words = [feature_names[idx] for idx in top_words_idx]
        print(f"\nTopic {i+1}:")
        print(", ".join(top_words))

# ä½¿ç”¨è¿™ä¸ªå‡½æ•°æ¥å¯è§†åŒ–LDAç»“æœ
visualize_lda_results(lda_model, count_vectorizer, lda_output)
```
# æƒ…æ„Ÿåˆ†æ

æ•°æ®å‡†å¤‡å’Œå¢å¼ºé¢„å¤„ç†



```js
import pandas as pd
import numpy as np
import re
import spacy
from spacy import displacy
from tqdm import tqdm
tqdm.pandas()

# åŠ è½½æ•°æ®å’Œæ¨¡å‹
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
df = pd.read_csv("preprocessed_news.csv")

# å¢å¼ºçš„æƒ…æ„Ÿåˆ†æé¢„å¤„ç†
def enhanced_sentiment_preprocess(text):
    """ä¿ç•™æƒ…æ„Ÿç›¸å…³è¯æ±‡å’Œä¸Šä¸‹æ–‡"""
    # æ¸…ç†æ–‡æœ¬
    text = re.sub(r"http\S+|www\S+|https\S+", "", str(text).lower())
    text = re.sub(r"[^a-zA-Z\s'â€™]", "", text)

    # å¤„ç†å¦å®šå’Œå¼ºè°ƒ
    text = re.sub(r"\b(not|never|no)\b", " not_", text)
    text = re.sub(r"\b(very|really|extremely)\b", " very_", text)

    # æå–æƒ…æ„Ÿç›¸å…³è¯æ±‡
    doc = nlp(text)
    tokens = []
    for token in doc:
        # ä¿ç•™å½¢å®¹è¯ã€å‰¯è¯ã€åŠ¨è¯ã€å¦å®šè¯ã€æƒ…æ„Ÿåè¯
        if (token.pos_ in {"ADJ", "ADV", "VERB", "NOUN"}) or \
           (token.dep_ == "neg") or \
           (token.text.startswith(("not_", "very_"))):
            tokens.append(token.lemma_)

    return " ".join(tokens)

# åº”ç”¨é¢„å¤„ç†
df["sentiment_text"] = df["text"].progress_apply(enhanced_sentiment_preprocess)
```

```js
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 892/892 [00:03<00:00, 229.00it/s]
```


å¤šæ¨¡å‹æƒ…æ„Ÿåˆ†æé›†æˆ

```js
# é¦–å…ˆç¡®ä¿å®‰è£…æ­£ç¡®ç‰ˆæœ¬çš„åº“

!pip install pandas vaderSentiment textblob transformers tqdm

# ç„¶åå¯¼å…¥åº“
import pandas as pd
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
from transformers import pipeline
from tqdm import tqdm
import numpy as np  # ç¡®ä¿è¿™æ˜¯1.24.0ç‰ˆæœ¬

print(f"ä½¿ç”¨çš„NumPyç‰ˆæœ¬: {np.__version__}")  # åº”è¯¥æ˜¾ç¤º1.24.x

tqdm.pandas()

# åˆå§‹åŒ–å„åˆ†ææ¨¡å‹
vader = SentimentIntensityAnalyzer()

# å®‰å…¨åœ°åˆå§‹åŒ–transformeræ¨¡å‹
try:
    sentiment_pipeline = pipeline("sentiment-analysis",
                                model="distilbert-base-uncased-finetuned-sst-2-english")
except Exception as e:
    print(f"æ— æ³•åŠ è½½transformeræ¨¡å‹: {e}")
    sentiment_pipeline = None

def ensemble_sentiment_analysis(text):
    """é›†æˆå¤šç§æƒ…æ„Ÿåˆ†ææ–¹æ³•"""
    # å¤„ç†ç¼ºå¤±å€¼
    if pd.isna(text):
        text = ""

    # VADERåˆ†æ
    vader_scores = vader.polarity_scores(text)

    # TextBlobåˆ†æ
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity

    # Transformeræ¨¡å‹åˆ†æ
    transformer_score = 0
    if sentiment_pipeline is not None:
        try:
            transformer_result = sentiment_pipeline(text[:512])[0]  # é™åˆ¶é•¿åº¦
            transformer_score = (1 if transformer_result["label"] == "POSITIVE" else -1) * transformer_result["score"]
        except Exception as e:
            print(f"Transformeråˆ†æå‡ºé”™: {e}")

    # åŠ æƒç»¼åˆå¾—åˆ†
    combined_score = (vader_scores["compound"] * 0.4 +
                     polarity * 0.3 +
                     transformer_score * 0.3)

    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    return {
        "vader_compound": vader_scores["compound"],
        "textblob_polarity": polarity,
        "transformer_score": transformer_score,
        "combined_score": combined_score,
        "sentiment_label": "positive" if combined_score > 0.1 else
                          "negative" if combined_score < -0.1 else
                          "neutral"
    }

# åº”ç”¨æƒ…æ„Ÿåˆ†æ
print("æ­£åœ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æ...")
sentiment_results = df["text"].progress_apply(ensemble_sentiment_analysis)
sentiment_df = pd.DataFrame(list(sentiment_results))
df = pd.concat([df, sentiment_df], axis=1)

print(df)
```

æ‰§è¡Œç»“æœ


```js
æ­£åœ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æ...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 892/892 [01:21<00:00, 10.89it/s]                                                  text        category  \
0    funniest tweet cat dog week sept dog dont unde...          COMEDY   
1    funniest tweet parent week sept accidentally p...       PARENTING   
2    maury will shortstop dodger maury will helped ...          SPORTS   
3    golden globe returning nbc january year past m...   ENTERTAINMENT   
4    biden say force defend taiwan china invaded pr...        POLITICS   
..                                                 ...             ...   
887  larry headline crystal nfl legend cruise isnt ...          TRAVEL   
888  really shouldnt eat gluten group expert seven ...        WELLNESS   
889  cant wait taking action alzheimers disease can...        WELLNESS   
890  catching sucking jill taking year retail exper...  STYLE & BEAUTY   
891  tip living life action based fear going give l...        WELLNESS   

                                        sentiment_text  vader_compound  \
0                 cat dog week sept dog understand eat          0.5574   
1    tweet parent week sept accidentally put grownu...          0.3400   
2    maury shortstop dodger maury help win world se...          0.0516   
3    return year past month hollywood effectively b...          0.0516   
4                            invade issue tension rise         -0.3182   
..                                                 ...             ...   
887  first rodeo though do cruise take look celebs ...          0.0000   
888  very _ eat gluten group expert country propose...         -0.4019   
889  wait take action alzheimer disease wait age bo...          0.7506   
890  catch suck jill take year retail experience ve...          0.0000   
891  tip living life action base fear go give long ...         -0.8834   

     textblob_polarity  transformer_score  combined_score sentiment_label  
0             0.000000           0.988167        0.519410        positive  
1             0.000000          -0.982850       -0.158855        negative  
2             0.800000           0.963742        0.549763        positive  
3             0.120833          -0.998164       -0.242559        negative  
4             0.000000           0.882423        0.137447        positive  
..                 ...                ...             ...             ...  
887           0.250000          -0.662286       -0.123686        negative  
888           0.200000          -0.998155       -0.400207        negative  
889           0.133333          -0.864603        0.080859         neutral  
890           0.350000           0.999054        0.404716        positive  
891           0.016667          -0.933136       -0.628301        negative  

[892 rows x 8 columns]
```

3.å®ä½“ä¸æ–¹é¢çº§æƒ…æ„Ÿåˆ†æ

```js
import spacy
from tqdm import tqdm
tqdm.pandas()

# åŠ è½½è‹±æ–‡æ¨¡å‹ï¼ˆä»…å¯ç”¨NERéœ€è¦çš„ç»„ä»¶ï¼‰
nlp_ner = spacy.load("en_core_web_sm", disable=["parser", "tagger", "lemmatizer"])

def extract_entities_with_sentiment(row):
    """
    ä¼˜åŒ–ç‰ˆçš„å®ä½“æƒ…æ„Ÿå…³è”æå–
    è¾“å…¥: DataFrameçš„ä¸€è¡Œ(åŒ…å«textå’Œæƒ…æ„Ÿåˆ†æ•°)
    è¾“å‡º: å®ä½“åŠå…¶æƒ…æ„Ÿä¿¡æ¯çš„å­—å…¸
    """
    text = row["text"]
    doc = nlp_ner(text)
    entities = {}
    
    for ent in doc.ents:
        if ent.label_ in ["PERSON", "ORG", "GPE", "PRODUCT"]:
            # è·å–å®ä½“ä¸Šä¸‹æ–‡çª—å£
            window_size = 10  # å‰åå„å–10ä¸ªè¯ä½œä¸ºä¸Šä¸‹æ–‡
            start = max(0, ent.start - window_size)
            end = min(len(doc), ent.end + window_size)
            context = doc[start:end].text
            
            # ä½¿ç”¨å·²æœ‰æƒ…æ„Ÿåˆ†æ•°(ä¼˜åŒ–æ€§èƒ½)
            entities[ent.text] = {
                "type": ent.label_,
                "sentiment": row["combined_score"],  # ä½¿ç”¨æ•´å¥æƒ…æ„Ÿåˆ†æ•°
                "label": row["sentiment_label"],
                "context": context,
                "entity_text": ent.text
            }
    
    return entities

# æµ‹è¯•ç¤ºä¾‹
sample_row = df.iloc[4]  # å–ç¬¬5æ¡æ•°æ®
print("ç¤ºä¾‹æ–‡æœ¬:", sample_row["text"])
print("æå–ç»“æœ:")
print(extract_entities_with_sentiment(sample_row))
```
ç»“æœ
```js
ç¤ºä¾‹æ–‡æœ¬: biden say force defend taiwan china invaded president issue vow tension china rise
æå–ç»“æœ:
{'taiwan': {'type': 'GPE', 'sentiment': np.float64(0.13744676992416383), 'label': 'positive', 'context': 'biden say force defend taiwan china invaded president issue vow tension china rise', 'entity_text': 'taiwan'}, 'china': {'type': 'GPE', 'sentiment': np.float64(0.13744676992416383), 'label': 'positive', 'context': 'say force defend taiwan china invaded president issue vow tension china rise', 'entity_text': 'china'}}
```

```js
# æ‰¹é‡å¤„ç†å‡½æ•°ï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰
def batch_extract_entities(df, batch_size=1000):
    """
    åˆ†æ‰¹å¤„ç†DataFrameï¼Œé¿å…å†…å­˜ä¸è¶³
    """
    results = []
    for i in tqdm(range(0, len(df), batch_size)):
        batch = df.iloc[i:i+batch_size]
        batch_results = batch.apply(extract_entities_with_sentiment, axis=1)
        results.extend(batch_results)
    return results

# åº”ç”¨åˆ°æ•´ä¸ªæ•°æ®é›†
print("æ­£åœ¨æå–å®ä½“åŠå…¶æƒ…æ„Ÿ...")
df["entity_sentiments"] = batch_extract_entities(df)
```
æ‰§è¡Œç»“æœ

```js
æ­£åœ¨æå–å®ä½“åŠå…¶æƒ…æ„Ÿ...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.67s/it]
```

æƒ…æ„Ÿåˆ†æå¯è§†åŒ–

```js
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from wordcloud import WordCloud

# è®¾ç½® seaborn ç»˜å›¾é£æ ¼
sns.set_style("whitegrid")  # è®¾ç½® seaborn çš„é£æ ¼ï¼Œè¿™é‡Œä½¿ç”¨ whitegrid é£æ ¼ï¼Œä½ ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©å…¶ä»–é£æ ¼
sns.set_palette("husl")



# 1. æ•´ä½“æƒ…æ„Ÿåˆ†å¸ƒ
plt.figure(figsize=(10, 6))
ax = sns.countplot(data=df, x='sentiment_label', 
                 order=['positive', 'neutral', 'negative'],
                 palette={'positive': '#4caf50', 'neutral': '#9e9e9e', 'negative': '#f44336'})
plt.title('News sentiment label distribution', fontsize=14, pad=20)
plt.xlabel('Sentiment classification', fontsize=12)
plt.ylabel('quantity', fontsize=12)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for p in ax.patches:
    ax.annotate(f'{p.get_height():.0f}', 
               (p.get_x() + p.get_width() / 2., p.get_height()),
               ha='center', va='center', 
               xytext=(0, 5), 
               textcoords='offset points')

plt.tight_layout()
plt.show()
```

![image](https://github.com/user-attachments/assets/094c38d8-c33b-4c77-a49c-36e8f7d45a7c)

```js
# 2. å¤šæ¨¡å‹æƒ…æ„Ÿåˆ†æ•°åˆ†å¸ƒå¯¹æ¯”
plt.figure(figsize=(14, 8))

# åˆ›å»ºå­å›¾ç½‘æ ¼
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# VADERåˆ†æ•°åˆ†å¸ƒ
sns.histplot(data=df, x='vader_compound', bins=30, kde=True, ax=axes[0, 0])
axes[0, 0].set_title('VADER sentiment score distribution')
axes[0, 0].axvline(0, color='k', linestyle='--')

# TextBlobåˆ†æ•°åˆ†å¸ƒ
sns.histplot(data=df, x='textblob_polarity', bins=30, kde=True, ax=axes[0, 1])
axes[0, 1].set_title('TextBlobsentiment score distribution')
axes[0, 1].axvline(0, color='k', linestyle='--')

# Transformeråˆ†æ•°åˆ†å¸ƒ
sns.histplot(data=df, x='transformer_score', bins=30, kde=True, ax=axes[1, 0])
axes[1, 0].set_title('Transformersentiment score distribution')
axes[1, 0].axvline(0, color='k', linestyle='--')

# ç»¼åˆåˆ†æ•°åˆ†å¸ƒ
sns.histplot(data=df, x='combined_score', bins=30, kde=True, ax=axes[1, 1])
axes[1, 1].set_title('Comprehensive sentiment score distribution')
axes[1, 1].axvline(0, color='k', linestyle='--')

plt.tight_layout()
plt.show()
```
![image](https://github.com/user-attachments/assets/6937d175-bd08-4fca-bb72-7c0e7b8fd595)

```js
# 3. å„ç±»åˆ«æƒ…æ„Ÿåˆ†å¸ƒ
plt.figure(figsize=(14, 8))

# æŒ‰ç±»åˆ«åˆ†ç»„è®¡ç®—å¹³å‡æƒ…æ„Ÿåˆ†æ•°
category_sentiment = df.groupby('category')['combined_score'].agg(['mean', 'count'])
category_sentiment = category_sentiment[category_sentiment['count'] > 50]  # è¿‡æ»¤æ ·æœ¬é‡å°‘çš„ç±»åˆ«

# ç»˜åˆ¶æ¡å½¢å›¾
ax = sns.barplot(data=category_sentiment.reset_index(), 
                x='mean', y='category',
                palette='coolwarm')
plt.axvline(0, color='k', linestyle='--')
plt.title('Average sentiment score of each news category', fontsize=14, pad=20)
plt.xlabel('average sentiment score', fontsize=12)
plt.ylabel('News categories', fontsize=12)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i, (mean, count) in enumerate(zip(category_sentiment['mean'], category_sentiment['count'])):
    ax.text(mean, i, f'{mean:.2f}\n(n={count})', 
            va='center', ha='left' if mean < 0 else 'right',
            color='white' if abs(mean) > 0.2 else 'black')

plt.tight_layout()
plt.show()
```
![image](https://github.com/user-attachments/assets/b24cc0e9-6fcb-4c03-b70f-aa58c131a376)

```js
# 4. æƒ…æ„Ÿè¯äº‘å¯¹æ¯”
def generate_sentiment_wordcloud(sentiment_type='positive', colormap='Greens'):
    """ç”Ÿæˆæƒ…æ„Ÿè¯äº‘"""
    subset = df[df['sentiment_label'] == sentiment_type]
    text = ' '.join(subset['sentiment_text'])
    
    # è¿‡æ»¤åœç”¨è¯
    stopwords = set(['say', 'said', 'will', 'one', 'year', 'new'])
    
    wordcloud = WordCloud(
        width=800, height=400,
        background_color="white",
        colormap=colormap,
        max_words=100,
        stopwords=stopwords,
        contour_width=3,
        contour_color='steelblue'
    ).generate(text)
    
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud)
    plt.title(f'{sentiment_type.capitalize()}Emotion word cloud', fontsize=14, pad=20)
    plt.axis("off")
    plt.show()

# ç”Ÿæˆæ­£é¢å’Œè´Ÿé¢è¯äº‘
generate_sentiment_wordcloud('positive', 'Greens')
generate_sentiment_wordcloud('negative', 'Reds')
```
![image](https://github.com/user-attachments/assets/391ca59c-67d6-4851-bbea-ddb87c429fea)

![image](https://github.com/user-attachments/assets/e1ad3297-f85d-4d1d-9d81-45e135308c94)

å®ä½“æƒ…æ„Ÿåˆ†æ

```js
# 5. å®ä½“æƒ…æ„Ÿåˆ†æ
from collections import defaultdict

# æå–æ‰€æœ‰å®ä½“åŠå…¶æƒ…æ„Ÿåˆ†æ•°
entity_sentiments = defaultdict(list)
for entities in df['entity_sentiments']:
    for ent, data in entities.items():
        entity_sentiments[ent].append(data['sentiment'])

# è®¡ç®—æ¯ä¸ªå®ä½“çš„å¹³å‡æƒ…æ„Ÿ
entity_avg_sentiment = {
    ent: np.mean(scores) 
    for ent, scores in entity_sentiments.items() 
    if len(scores) >= 5  # åªè€ƒè™‘å‡ºç°5æ¬¡ä»¥ä¸Šçš„å®ä½“
}

# è½¬æ¢ä¸ºDataFrame
entity_df = pd.DataFrame({
    'entity': list(entity_avg_sentiment.keys()),
    'avg_sentiment': list(entity_avg_sentiment.values()),
    'count': [len(entity_sentiments[ent]) for ent in entity_avg_sentiment.keys()]
})

# ç­›é€‰æœ€å…·ä»£è¡¨æ€§çš„å®ä½“
top_entities = entity_df.nlargest(10, 'count')

# å¯è§†åŒ–
plt.figure(figsize=(14, 8))
ax = sns.barplot(data=top_entities, x='avg_sentiment', y='entity',
                palette='coolwarm')
plt.axvline(0, color='k', linestyle='--')
plt.title('High-frequency entity sentiment analysis (occurrence â‰¥ 5 times', fontsize=14, pad=20)
plt.xlabel('average sentiment score', fontsize=12)
plt.ylabel('Entity name', fontsize=12)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for p in ax.patches:
    width = p.get_width()
    ax.annotate(f'{width:.2f}',
                (width, p.get_y() + p.get_height() / 2.),
                ha='left' if width > 0 else 'right',
                va='center',
                xytext=(5 if width > 0 else -5, 0),
                textcoords='offset points')

plt.tight_layout()
plt.show()
```
![image](https://github.com/user-attachments/assets/822b6ea2-f09b-4430-b3b8-6a62ea9cf1a4)

```js
from ipywidgets import interact, widgets
import plotly.express as px

# 6.1 äº¤äº’å¼æƒ…æ„Ÿæ¢ç´¢
@interact
def explore_sentiment(category=widgets.Dropdown(
    options=sorted(df['category'].unique())),
    score_type=widgets.Dropdown(
    options=['combined_score', 'vader_compound', 'textblob_polarity'])):
    """äº¤äº’å¼æ¢ç´¢ä¸åŒç±»åˆ«çš„æƒ…æ„Ÿåˆ†å¸ƒ"""
    subset = df[df['category'] == category]
    
    fig = px.histogram(subset, x=score_type, 
                      nbins=30,
                      title=f'{category} category {score_type} score_type',
                      color_discrete_sequence=['#636EFA'])
    
    fig.update_layout(
        xaxis_title='sentiment score',
        yaxis_title='quantity',
        bargap=0.1
    )
    fig.add_vline(x=0, line_dash="dash", line_color="black")
    fig.show()

# 6.2 äº¤äº’å¼å®ä½“æ¢ç´¢
if len(entity_df) > 0:
    @interact
    def explore_entity(min_count=widgets.IntSlider(
        min=1, max=entity_df['count'].max(), value=5)):
        """äº¤äº’å¼æ¢ç´¢å®ä½“æƒ…æ„Ÿ"""
        filtered = entity_df[entity_df['count'] >= min_count]
        
        fig = px.scatter(filtered, x='avg_sentiment', y='entity',
                        size='count', color='avg_sentiment',
                        color_continuous_scale='RdYlGn',
                        title=f'Entity sentiment analysis (occurrence â‰¥{min_count} Second - rate)')
        
        fig.update_layout(
            xaxis_title='average sentiment score',
            yaxis_title='Entity name',
            coloraxis_colorbar=dict(title="sentiment score")
        )
        fig.add_vline(x=0, line_dash="dash", line_color="black")
        fig.show()
```
![image](https://github.com/user-attachments/assets/22b2bd7e-b3b0-4602-91b0-fcc3444aeebd)



![image](https://github.com/user-attachments/assets/26682df9-1b40-4582-837f-73a615be3233)

```js
# 7. ç”Ÿæˆé«˜çº§åˆ†ææŠ¥å‘Š
def generate_sentiment_report():
    """ç”Ÿæˆç»¼åˆæƒ…æ„Ÿåˆ†ææŠ¥å‘Š"""
    report = {
        'overall_sentiment': {
            'positive': len(df[df['sentiment_label'] == 'positive']),
            'neutral': len(df[df['sentiment_label'] == 'neutral']),
            'negative': len(df[df['sentiment_label'] == 'negative'])
        },
        'top_positive_category': df.groupby('category')['combined_score'].mean().idxmax(),
        'top_negative_category': df.groupby('category')['combined_score'].mean().idxmin(),
        'most_positive_entity': max(entity_avg_sentiment.items(), key=lambda x: x[1])[0] if entity_avg_sentiment else "N/A",
        'most_negative_entity': min(entity_avg_sentiment.items(), key=lambda x: x[1])[0] if entity_avg_sentiment else "N/A"
    }
    
    # æ‰“å°æŠ¥å‘Š
    print("="*50)
    print("æ–°é—»æƒ…æ„Ÿåˆ†æç»¼åˆæŠ¥å‘Š".center(40))
    print("="*50)
    print(f"\næ•´ä½“æƒ…æ„Ÿåˆ†å¸ƒ:")
    print(f"- æ­£é¢æ–°é—»: {report['overall_sentiment']['positive']}ç¯‡")
    print(f"- ä¸­æ€§æ–°é—»: {report['overall_sentiment']['neutral']}ç¯‡")
    print(f"- è´Ÿé¢æ–°é—»: {report['overall_sentiment']['negative']}ç¯‡")
    
    print(f"\næœ€å…·æ­£é¢æƒ…æ„Ÿçš„æ–°é—»ç±»åˆ«: {report['top_positive_category']}")
    print(f"æœ€å…·è´Ÿé¢æƒ…æ„Ÿçš„æ–°é—»ç±»åˆ«: {report['top_negative_category']}")
    
    if entity_avg_sentiment:
        print(f"\næœ€æ­£é¢å…³è”å®ä½“: {report['most_positive_entity']}")
        print(f"æœ€è´Ÿé¢å…³è”å®ä½“: {report['most_negative_entity']}")
    
    print("\n" + "="*50)

# ç”ŸæˆæŠ¥å‘Š
generate_sentiment_report()
```


ç»“æœ

```js
==================================================
æ–°é—»æƒ…æ„Ÿåˆ†æç»¼åˆæŠ¥å‘ŠNews Sentiment Analysis Comprehensive Report
==================================================

æ•´ä½“æƒ…æ„Ÿåˆ†å¸ƒOverall sentiment distribution:
- æ­£é¢æ–°é—»Positive News: 317ç¯‡
- ä¸­æ€§æ–°é—»Neutral News: 94ç¯‡
- è´Ÿé¢æ–°é—»Negative News: 481ç¯‡

æœ€å…·æ­£é¢æƒ…æ„Ÿçš„æ–°é—»ç±»åˆ«News categories with the most positive sentiment: FOOD & DRINK
æœ€å…·è´Ÿé¢æƒ…æ„Ÿçš„æ–°é—»ç±»åˆ«News categories with the most negative sentiment: POLITICS

æœ€æ­£é¢å…³è”å®ä½“Most positively associated entity: los angeles
æœ€è´Ÿé¢å…³è”å®ä½“Most Negatively Associated Entity: texas

==================================================
```
å•†ä¸šåº”ç”¨åœºæ™¯å®ç°


5.1 ç«äº‰å®ä½“å¯¹æ¯”åˆ†æ


```js
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from collections import defaultdict

def compare_entities(df, entity1=None, entity2=None):
    """
    æ¯”è¾ƒä¸¤ä¸ªå®ä½“çš„æƒ…æ„Ÿè¡¨ç°
    å‚æ•°:
        df: åŒ…å«å®ä½“æƒ…æ„Ÿæ•°æ®çš„DataFrame
        entity1: ç¬¬ä¸€ä¸ªå®ä½“åç§° (å¯é€‰)
        entity2: ç¬¬äºŒä¸ªå®ä½“åç§° (å¯é€‰)
    """
    # æ£€æŸ¥entity_sentimentsåˆ—æ˜¯å¦å­˜åœ¨
    if "entity_sentiments" not in df.columns:
        print("æ•°æ®ä¸­ç¼ºå°‘ 'entity_sentiments' åˆ—")
        return
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šå®ä½“ï¼Œè‡ªåŠ¨é€‰æ‹©æ•°æ®ä¸­æœ€å¸¸è§çš„ä¸¤ä¸ªå®ä½“
    if entity1 is None or entity2 is None:
        entity_counts = defaultdict(int)
        for entities in df["entity_sentiments"]:
            if isinstance(entities, dict):
                for ent in entities.keys():
                    entity_counts[ent] += 1
        
        if len(entity_counts) < 2:
            print("æ•°æ®ä¸­å¯æ¯”è¾ƒçš„å®ä½“ä¸è¶³ (è‡³å°‘éœ€è¦2ä¸ªä¸åŒå®ä½“)")
            return
        
        # é€‰æ‹©å‡ºç°é¢‘ç‡æœ€é«˜çš„ä¸¤ä¸ªä¸åŒå®ä½“
        top_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:2]
        entity1, entity2 = top_entities[0][0], top_entities[1][0]
        print(f"è‡ªåŠ¨é€‰æ‹©å®ä½“: {entity1} vs {entity2}")
    
    # æ”¶é›†å®ä½“æƒ…æ„Ÿæ•°æ®
    entity1_data = []
    entity2_data = []
    
    for _, row in df.iterrows():
        entities = row["entity_sentiments"]
        if isinstance(entities, dict):
            if entity1 in entities:
                entity1_data.append(entities[entity1].get("sentiment", 0))
            if entity2 in entities:
                entity2_data.append(entities[entity2].get("sentiment", 0))
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿæ•°æ®
    if not entity1_data or not entity2_data:
        print(f"æ²¡æœ‰è¶³å¤Ÿæ•°æ®è¿›è¡Œæ¯”è¾ƒ (å®ä½“1: {len(entity1_data)}æ¡, å®ä½“2: {len(entity2_data)}æ¡)")
        return
    
    # å¯è§†åŒ–æ¯”è¾ƒ
    plt.figure(figsize=(14, 6))
    
    # å­å›¾1: æƒ…æ„Ÿåˆ†å¸ƒç›´æ–¹å›¾
    plt.subplot(1, 2, 1)
    sns.histplot(entity1_data, bins=15, color="blue", kde=True, alpha=0.6, label=entity1)
    sns.histplot(entity2_data, bins=15, color="orange", kde=True, alpha=0.6, label=entity2)
    plt.axvline(np.mean(entity1_data), color="blue", linestyle="--")
    plt.axvline(np.mean(entity2_data), color="orange", linestyle="--")
    plt.title(f"{entity1} vs {entity2} æƒ…æ„Ÿåˆ†å¸ƒæ¯”è¾ƒ")
    plt.xlabel("æƒ…æ„Ÿåˆ†æ•°")
    plt.legend()
    
    # å­å›¾2: ç®±çº¿å›¾æ¯”è¾ƒ
    plt.subplot(1, 2, 2)
    plot_data = pd.DataFrame({
        "Entity": [entity1]*len(entity1_data) + [entity2]*len(entity2_data),
        "Sentiment": entity1_data + entity2_data
    })
    sns.boxplot(data=plot_data, x="Entity", y="Sentiment", 
                palette={"blue", "orange"}, width=0.4)
    plt.title("æƒ…æ„Ÿåˆ†æ•°ç®±çº¿å›¾æ¯”è¾ƒ")
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\n{entity1} æƒ…æ„Ÿåˆ†æç»“æœ:")
    print(f"- å¹³å‡åˆ†: {np.mean(entity1_data):.2f}")
    print(f"- æ ‡å‡†å·®: {np.std(entity1_data):.2f}")
    print(f"- æ•°æ®ç‚¹: {len(entity1_data)}æ¡")
    
    print(f"\n{entity2} æƒ…æ„Ÿåˆ†æç»“æœ:")
    print(f"- å¹³å‡åˆ†: {np.mean(entity2_data):.2f}")
    print(f"- æ ‡å‡†å·®: {np.std(entity2_data):.2f}")
    print(f"- æ•°æ®ç‚¹: {len(entity2_data)}æ¡")
    
    # æ‰§è¡Œtæ£€éªŒ (å¦‚æœæ•°æ®è¶³å¤Ÿ)
    if len(entity1_data) > 1 and len(entity2_data) > 1:
        from scipy import stats
        t_stat, p_value = stats.ttest_ind(entity1_data, entity2_data)
        print(f"\nç‹¬ç«‹æ ·æœ¬tæ£€éªŒç»“æœ:")
        print(f"- tç»Ÿè®¡é‡: {t_stat:.2f}")
        print(f"- på€¼: {p_value:.4f}")
        if p_value < 0.05:
            print("-> å·®å¼‚æ˜¾è‘— (p < 0.05)")
        else:
            print("-> å·®å¼‚ä¸æ˜¾è‘—")

# ä½¿ç”¨ç¤ºä¾‹
print("="*50)
print("å®ä½“æƒ…æ„Ÿæ¯”è¾ƒåˆ†æ")
print("="*50)

# æ–¹å¼1: è‡ªåŠ¨é€‰æ‹©æœ€å¸¸è§çš„ä¸¤ä¸ªå®ä½“æ¯”è¾ƒ
compare_entities(df)

# æ–¹å¼2: æ‰‹åŠ¨æŒ‡å®šå®ä½“æ¯”è¾ƒ (æ ¹æ®æ‚¨çš„å®é™…æ•°æ®ä¸­çš„å®ä½“åç§°)
# compare_entities(df, entity1="funniest tweet parent", entity2="grownup toothpaste")
```

ç»“æœ

![image](https://github.com/user-attachments/assets/f19301fb-f0fe-4185-9bf3-c1dd8597907c)


```js

gop Sentiment analysis results:
- average score: -0.20
- standard deviation: 0.48
- data points: 32æ¡

donald trump Sentiment analysis results:
- average score: -0.12
- standard deviation: 0.37
- data points: 18æ¡

 Independent samples t-test results:
- t-statistic: -0.62
- p value: 0.5413
-> The difference is not significant
```

5.2 èˆ†æƒ…é¢„è­¦ç³»ç»Ÿ

```js
def sentiment_alert_system(df, threshold=-0.2):
    """
    è´Ÿé¢èˆ†æƒ…é¢„è­¦ç³»ç»Ÿ
    å‚æ•°:
        df: åŒ…å«æƒ…æ„Ÿåˆ†æç»“æœçš„DataFrame
        threshold: è´Ÿé¢æƒ…æ„Ÿé˜ˆå€¼ (é»˜è®¤-0.2)
    """
    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
    required_columns = ['entity_sentiments']
    missing_cols = [col for col in required_columns if col not in df.columns]

    if missing_cols:
        print(f"âš ï¸ Missing required columns in data: {missing_cols}")
        return

    # å°è¯•ä¸åŒçš„æƒ…æ„Ÿåˆ†æ•°åˆ— (æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥)
    score_columns = ['combined_score', 'vader_compound', 'textblob_polarity']
    score_col = next((col for col in score_columns if col in df.columns), None)

    if score_col is None:
        print("âš ï¸ No available sentiment score columns found in data")
        return

    print(f"Using '{score_col}' as sentiment score reference")

    # æŒ‰ç±»åˆ«æ£€æµ‹è´Ÿé¢æƒ…ç»ª
    negative_news = df[df[score_col] < threshold]

    if len(negative_news) > 0:
        print(f"\nâš ï¸ Detected {len(negative_news)} negative news items (score < {threshold})")

        # ç»Ÿè®¡é«˜é¢‘è´Ÿé¢å®ä½“
        negative_entities = {}
        for entities in negative_news["entity_sentiments"]:
            # ç¡®ä¿entity_sentimentsæ˜¯å­—å…¸æ ¼å¼
            if isinstance(entities, str):
                try:
                    entities = eval(entities)  # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—å…¸
                except:
                    continue

            if isinstance(entities, dict):
                for ent, data in entities.items():
                    if isinstance(data, dict) and 'sentiment' in data:
                        if data['sentiment'] < threshold:
                            negative_entities[ent] = negative_entities.get(ent, 0) + 1

        # å±•ç¤ºTopè´Ÿé¢å®ä½“
        if negative_entities:
            top_negative = sorted(negative_entities.items(),
                                 key=lambda x: x[1], reverse=True)[:5]
            print("\nğŸ”¥ Top negative associated entities:")
            for ent, count in top_negative:
                print(f"- {ent}: {count} times")
        else:
            print("\nNo significant negative associated entities found")

        # å±•ç¤ºä»£è¡¨æ€§è´Ÿé¢æ–°é—»
        print("\nğŸ“Œ Representative negative news samples:")
        sample_size = min(3, len(negative_news))
        for idx, row in negative_news.nsmallest(sample_size, score_col).iterrows():
            print(f"\n[{idx}] Sentiment score: {row[score_col]:.2f}")

            # å°è¯•è·å–æ ‡é¢˜æˆ–æ–‡æœ¬ç‰‡æ®µ
            if 'headline' in df.columns:
                print(f"Headline: {row['headline']}")
            elif 'text' in df.columns:
                preview = row['text'][:50] + "..." if len(row['text']) > 50 else row['text']
                print(f"Content: {preview}")

            # æ˜¾ç¤ºå…³è”å®ä½“
            entities = row['entity_sentiments']
            if isinstance(entities, str):
                try:
                    entities = eval(entities)
                except:
                    entities = {}

            if isinstance(entities, dict):
                print("Key entities:", ", ".join(entities.keys()))
    else:
        print("\nâœ… No significant negative sentiment detected")

# ä½¿ç”¨ç¤ºä¾‹
print("="*50)
print("Negative Sentiment Alert System Started")
print("="*50)
sentiment_alert_system(df)  # ä¼ å…¥æ‚¨çš„DataFrame
```


jieguo
```js
==================================================
Negative Sentiment Alert System Started
==================================================
Using 'combined_score' as sentiment score reference

âš ï¸ Detected 402 negative news items (score < -0.2)

ğŸ”¥ Top negative associated entities:
- gop: 22 times
- texas: 12 times
- senate: 11 times
- white house: 9 times
- donald trump: 9 times

ğŸ“Œ Representative negative news samples:

[70] Sentiment score: -0.95
Content: jared kushner blast nasty troll chrissy teigen att...
Key entities: jared kushner, troll

[539] Sentiment score: -0.91
Content: cory booker explains went bat ketanji brown jackso...
Key entities: cory booker, ketanji brown jackson, dick durbin, gop

[797] Sentiment score: -0.90
Content: exnfl player say violently arrested cop mistook ph...
Key entities: desmond marrow
```
æŠ€æœ¯æŒ‘æˆ˜è§£å†³æ–¹æ¡ˆ
6.1 é«˜çº§è®½åˆºæ£€æµ‹

```js
from transformers import pipeline

# åŠ è½½è®½åˆºæ£€æµ‹æ¨¡å‹
irony_pipeline = pipeline("text-classification",
                         model="cardiffnlp/twitter-roberta-base-irony")

def detect_irony(text):
    """ä½¿ç”¨Transformeræ¨¡å‹æ£€æµ‹è®½åˆº"""
    try:
        result = irony_pipeline(text[:512])[0]  # é™åˆ¶é•¿åº¦
        return result["label"] == "irony"
    except:
        return False

# åº”ç”¨åˆ°ç–‘ä¼¼è®½åˆºæ–‡æœ¬
potential_irony = df[(df["vader_compound"] > 0) & (df["textblob_polarity"] < 0)]
if len(potential_irony) > 0:
    potential_irony["is_ironic"] = potential_irony["text"].progress_apply(detect_irony)
    print(f"Detected {potential_irony['is_ironic'].sum()} ironic news items")
```
ç»“æœ

```js
config.json:â€‡100%
â€‡705/705â€‡[00:00<00:00,â€‡24.3kB/s]
pytorch_model.bin:â€‡100%
â€‡499M/499Mâ€‡[00:06<00:00,â€‡130MB/s]
model.safetensors:â€‡100%
â€‡499M/499Mâ€‡[00:08<00:00,â€‡72.0MB/s]
vocab.json:â€‡100%
â€‡899k/899kâ€‡[00:00<00:00,â€‡5.03MB/s]
merges.txt:â€‡100%
â€‡456k/456kâ€‡[00:00<00:00,â€‡6.88MB/s]
special_tokens_map.json:â€‡100%
â€‡150/150â€‡[00:00<00:00,â€‡4.83kB/s]
Device set to use cpu

  0%|          | 0/63 [00:00<?, ?it/s]
  3%|â–         | 2/63 [00:00<00:10,  5.70it/s]
  5%|â–         | 3/63 [00:00<00:12,  4.85it/s]
  6%|â–‹         | 4/63 [00:00<00:13,  4.32it/s]
  8%|â–Š         | 5/63 [00:01<00:13,  4.19it/s]
 10%|â–‰         | 6/63 [00:01<00:13,  4.22it/s]
 11%|â–ˆ         | 7/63 [00:01<00:12,  4.33it/s]
 13%|â–ˆâ–        | 8/63 [00:01<00:12,  4.28it/s]
 14%|â–ˆâ–        | 9/63 [00:02<00:13,  3.98it/s]
 16%|â–ˆâ–Œ        | 10/63 [00:02<00:14,  3.69it/s]
 17%|â–ˆâ–‹        | 11/63 [00:02<00:16,  3.19it/s]
 19%|â–ˆâ–‰        | 12/63 [00:03<00:22,  2.22it/s]
 21%|â–ˆâ–ˆ        | 13/63 [00:04<00:35,  1.41it/s]
 22%|â–ˆâ–ˆâ–       | 14/63 [00:05<00:39,  1.23it/s]
 24%|â–ˆâ–ˆâ–       | 15/63 [00:06<00:41,  1.16it/s]
 25%|â–ˆâ–ˆâ–Œ       | 16/63 [00:07<00:37,  1.25it/s]
 27%|â–ˆâ–ˆâ–‹       | 17/63 [00:07<00:29,  1.56it/s]
 29%|â–ˆâ–ˆâ–Š       | 18/63 [00:08<00:22,  1.96it/s]
 30%|â–ˆâ–ˆâ–ˆ       | 19/63 [00:08<00:19,  2.21it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 20/63 [00:08<00:18,  2.28it/s]
 33%|â–ˆâ–ˆâ–ˆâ–      | 21/63 [00:09<00:17,  2.39it/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 22/63 [00:09<00:14,  2.83it/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 23/63 [00:09<00:14,  2.80it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/63 [00:10<00:15,  2.52it/s]
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 25/63 [00:10<00:14,  2.65it/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/63 [00:10<00:14,  2.55it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/63 [00:11<00:15,  2.33it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/63 [00:11<00:14,  2.37it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/63 [00:12<00:13,  2.54it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 30/63 [00:12<00:12,  2.75it/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 31/63 [00:12<00:12,  2.64it/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 32/63 [00:13<00:11,  2.82it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 33/63 [00:13<00:10,  2.96it/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/63 [00:13<00:09,  2.97it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 35/63 [00:14<00:09,  2.86it/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 36/63 [00:14<00:10,  2.59it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 37/63 [00:15<00:10,  2.55it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 38/63 [00:16<00:14,  1.78it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 39/63 [00:16<00:15,  1.57it/s]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 40/63 [00:18<00:18,  1.24it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 41/63 [00:18<00:16,  1.34it/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 42/63 [00:19<00:14,  1.46it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 43/63 [00:19<00:11,  1.72it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/63 [00:19<00:09,  1.98it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 45/63 [00:20<00:08,  2.07it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 46/63 [00:20<00:08,  2.12it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 47/63 [00:21<00:07,  2.26it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/63 [00:21<00:06,  2.32it/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 49/63 [00:21<00:05,  2.45it/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 50/63 [00:22<00:04,  2.64it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 51/63 [00:22<00:04,  2.78it/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 52/63 [00:22<00:03,  3.00it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 53/63 [00:22<00:02,  3.51it/s]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 54/63 [00:23<00:02,  3.93it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 55/63 [00:23<00:01,  4.76it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 56/63 [00:23<00:01,  4.93it/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 57/63 [00:23<00:01,  4.71it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 58/63 [00:23<00:01,  4.74it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 59/63 [00:24<00:00,  4.70it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 60/63 [00:24<00:00,  4.79it/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 61/63 [00:24<00:00,  4.63it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 62/63 [00:24<00:00,  4.69it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:25<00:00,  2.50it/s]Detected 36 ironic news items

<ipython-input-25-9f0eea376282>:18: SettingWithCopyWarning:


A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
```

![image](https://github.com/user-attachments/assets/94a59fe6-82e4-48ed-b137-c0d0b385ba59)


6.2 åŠ¨æ€é¢†åŸŸè‡ªé€‚åº”


```js
def dynamic_domain_adaptation(text, category):
    """åŠ¨æ€é¢†åŸŸé€‚åº”æƒ…æ„Ÿåˆ†æ"""
    # é¢†åŸŸç‰¹å®šè°ƒæ•´è§„åˆ™
    domain_rules = {
        "POLITICS": {
            "boost_words": ["reform", "progress", "bipartisan"],
            "penalty_words": ["scandal", "corruption", "controversy"]
        },
        "TECH": {
            "boost_words": ["innovative", "sleek", "user-friendly"],
            "penalty_words": ["buggy", "outdated", "overpriced"]
        }
    }

    # è·å–åŸºç¡€åˆ†æ•°
    base_score = ensemble_sentiment_analysis(text)["combined_score"]

    # åº”ç”¨é¢†åŸŸè°ƒæ•´
    if category in domain_rules:
        boost = sum(1 for word in domain_rules[category]["boost_words"]
                if word in text.lower()) * 0.05
        penalty = sum(1 for word in domain_rules[category]["penalty_words"]
                     if word in text.lower()) * 0.05
        adjusted_score = base_score + boost - penalty
        return max(-1, min(1, adjusted_score))  # ä¿æŒåœ¨[-1,1]èŒƒå›´å†…

    return base_score

# åº”ç”¨é¢†åŸŸè‡ªé€‚åº”
df["domain_adjusted_score"] = df.progress_apply(
    lambda row: dynamic_domain_adaptation(row["text"], row["category"]), axis=1)
```


ç»“æœ

ç³»ç»Ÿé›†æˆä¸éƒ¨ç½²

```js
import pandas as pd
import spacy
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from transformers import pipeline

# ä¸‹è½½ vader_lexicon èµ„æº
nltk.download('vader_lexicon')

# å‡è®¾è¿™äº›å‡½æ•°å·²ç»å®šä¹‰
def enhanced_sentiment_preprocess(text):
    return text

def ensemble_sentiment_analysis(text):
    return "positive"

def extract_entities_with_sentiment(text):
    """æå–å®ä½“åŠå…¶æƒ…æ„Ÿä¿¡æ¯"""
    nlp_ner = spacy.load("en_core_web_sm")
    doc = nlp_ner(text)
    entities = {}
    for ent in doc.ents:
        entities[ent.text] = {
            "label": ent.label_,
            # è¿™é‡Œç®€å•å‡è®¾æƒ…æ„Ÿéƒ½æ˜¯ä¸­æ€§ï¼Œå®é™…éœ€è¦æ ¹æ®æƒ…å†µå®ç°æƒ…æ„Ÿåˆ†æ
            "sentiment": "neutral"
        }
    return entities

def detect_irony(text):
    return False


class NewsSentimentAnalyzer:
    def __init__(self, data_path):
        """åˆå§‹åŒ–åˆ†æç³»ç»Ÿ"""
        self.df = self._load_data(data_path)
        self._load_models()

    def _load_data(self, path):
        """åŠ è½½æ•°æ®"""
        df = pd.read_csv(path)
        return df

    def _load_models(self):
        """åŠ è½½æ‰€éœ€æ¨¡å‹"""
        print("Loading NLP models...")
        self.nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
        self.nlp_ner = spacy.load("en_core_web_sm")
        self.vader = SentimentIntensityAnalyzer()
        self.sentiment_pipeline = pipeline(
            "sentiment-analysis",
            model="distilbert-base-uncased-finetuned-sst-2-english"
        )
        self.irony_pipeline = pipeline(
            "text-classification",
            model="cardiffnlp/twitter-roberta-base-irony"
        )

    def analyze(self, text):
        """åˆ†æå•æ¡æ–°é—»"""
        # é¢„å¤„ç†
        clean_text = enhanced_sentiment_preprocess(text)

        # æƒ…æ„Ÿåˆ†æ
        sentiment = ensemble_sentiment_analysis(text)

        # å®ä½“æå–
        entities = extract_entities_with_sentiment(text)

        # è®½åˆºæ£€æµ‹
        is_ironic = detect_irony(text)

        return {
            "text": text,
            "clean_text": clean_text,
            "sentiment": sentiment,
            "entities": entities,
            "is_ironic": is_ironic
        }

    def batch_analyze(self, texts):
        """æ‰¹é‡åˆ†ææ–°é—»"""
        return [self.analyze(text) for text in texts]

    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        # å®ç°æŠ¥å‘Šç”Ÿæˆé€»è¾‘...
        pass

# ä½¿ç”¨ç¤ºä¾‹
analyzer = NewsSentimentAnalyzer("preprocessed_news.csv")
df = analyzer.df
sample_news = df.iloc[0]["text"]
print(analyzer.analyze(sample_news))
```
ç»“æœ

```js
[nltk_data] Downloading package vader_lexicon to /root/nltk_data...
Loading NLP models...
Device set to use cpu
Device set to use cpu
{'text': 'funniest tweet cat dog week sept dog dont understand eaten', 'clean_text': 'funniest tweet cat dog week sept dog dont understand eaten', 'sentiment': 'positive', 'entities': {'week sept': {'label': 'DATE', 'sentiment': 'neutral'}}, 'is_ironic': False}
```

```js
import pandas as pd
import spacy
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from transformers import pipeline

# ä¸‹è½½ vader_lexicon èµ„æº
nltk.download('vader_lexicon')

# é‡æ–°åŠ è½½ spacy æ¨¡å‹ï¼Œä¸ç¦ç”¨è§£æå™¨
nlp = spacy.load("en_core_web_sm")


def enhanced_sentiment_preprocess(text):
    return text


def ensemble_sentiment_analysis(text):
    return "positive"


def extract_entities_with_sentiment(text):
    """æå–å®ä½“åŠå…¶æƒ…æ„Ÿä¿¡æ¯"""
    doc = nlp(text)
    entities = {}
    for ent in doc.ents:
        entities[ent.text] = {
            "label": ent.label_,
            # è¿™é‡Œç®€å•å‡è®¾æƒ…æ„Ÿéƒ½æ˜¯ä¸­æ€§ï¼Œå®é™…éœ€è¦æ ¹æ®æƒ…å†µå®ç°æƒ…æ„Ÿåˆ†æ
            "sentiment": "neutral"
        }
    return entities


def detect_irony(text):
    return False


def extract_entities_and_aspects(text):
    doc = nlp(text)
    entities = []
    aspects = []

    # æå–å‘½åå®ä½“(äº§å“ã€ç»„ç»‡ã€åœ°ç‚¹ç­‰)
    for ent in doc.ents:
        if ent.label_ in ('ORG', 'PRODUCT', 'PERSON', 'GPE'):
            entities.append((ent.text, ent.label_))

    # æå–æ–¹é¢(åè¯çŸ­è¯­)
    for chunk in doc.noun_chunks:
        # è¿‡æ»¤é€šç”¨åè¯
        if chunk.root.pos_ == 'NOUN' and len(chunk.text) > 3:
            aspects.append(chunk.text)

    return list(set(entities)), list(set(aspects))


class NewsSentimentAnalyzer:
    def __init__(self, data_path):
        """åˆå§‹åŒ–åˆ†æç³»ç»Ÿ"""
        self.df = self._load_data(data_path)
        self._load_models()

    def _load_data(self, path):
        """åŠ è½½æ•°æ®"""
        df = pd.read_csv(path)
        return df

    def _load_models(self):
        """åŠ è½½æ‰€éœ€æ¨¡å‹"""
        print("Loading NLP models...")
        self.nlp = spacy.load("en_core_web_sm")
        self.nlp_ner = spacy.load("en_core_web_sm")
        self.vader = SentimentIntensityAnalyzer()
        self.sentiment_pipeline = pipeline(
            "sentiment-analysis",
            model="distilbert-base-uncased-finetuned-sst-2-english"
        )
        self.irony_pipeline = pipeline(
            "text-classification",
            model="cardiffnlp/twitter-roberta-base-irony"
        )

    def analyze(self, text):
        """åˆ†æå•æ¡æ–°é—»"""
        # é¢„å¤„ç†
        clean_text = enhanced_sentiment_preprocess(text)

        # æƒ…æ„Ÿåˆ†æ
        sentiment = ensemble_sentiment_analysis(text)

        # å®ä½“æå–
        entities = extract_entities_with_sentiment(text)

        # è®½åˆºæ£€æµ‹
        is_ironic = detect_irony(text)

        return {
            "text": text,
            "clean_text": clean_text,
            "sentiment": sentiment,
            "entities": entities,
            "is_ironic": is_ironic
        }

    def batch_analyze(self, texts):
        """æ‰¹é‡åˆ†ææ–°é—»"""
        return [self.analyze(text) for text in texts]

    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        # å®ç°æŠ¥å‘Šç”Ÿæˆé€»è¾‘...
        pass


# ä½¿ç”¨ç¤ºä¾‹
analyzer = NewsSentimentAnalyzer("preprocessed_news.csv")
df = analyzer.df
sample_text = "The new iPhone's battery life is disappointing compared to Samsung's latest model."
entities, aspects = extract_entities_and_aspects(sample_text)
print("Entities:", entities)
print("Aspects:", aspects)

# åº”ç”¨åˆ°æ•´ä¸ªæ•°æ®é›†(æŠ½æ ·éƒ¨åˆ†æ•°æ®)
df[['entities', 'aspects']] = df['text'].apply(
    lambda x: pd.Series(extract_entities_and_aspects(x)))
```

ç»“æœ

```js
[nltk_data] Downloading package vader_lexicon to /root/nltk_data...
[nltk_data]   Package vader_lexicon is already up-to-date!
Loading NLP models...
Device set to use cpu
Device set to use cpu
Entities: [('iPhone', 'ORG'), ('Samsung', 'ORG')]
Aspects: ["Samsung's latest model", "The new iPhone's battery life"]
```
# å…³è”è§„åˆ™æŒ–æ˜


```js
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder

def prepare_transaction_data(df):
    """
    æŒ‰è¯¾ç¨‹æ ‡å‡†å‡†å¤‡äº‹åŠ¡æ•°æ®
    ä¿®æ”¹ç‚¹ï¼š
    1. ç§»é™¤ä¸å¿…è¦çš„è¿”å›é¡¹
    2. ç®€åŒ–ç‰¹å¾æå–é€»è¾‘
    3. ç¡®ä¿è¾“å‡ºæ ¼å¼ä¸è¯¾ç¨‹ç¤ºä¾‹å®Œå…¨ä¸€è‡´
    """
    transactions = []
    for _, row in df.iterrows():
        itemset = []
        
        # å¿…é¡»åŒ…å«æƒ…æ„Ÿæ ‡ç­¾ï¼ˆè¯¾ç¨‹ä¸­çš„Yå˜é‡ï¼‰
        itemset.append(f"sentiment={row['sentiment_label']}")
        
        # ä»…ä¿ç•™è¯¾ç¨‹è¦æ±‚çš„ç‰¹å¾ç±»å‹
        if 'category' in df.columns:
            itemset.append(f"category={row['category']}")
        
        if 'entity_sentiments' in df.columns and isinstance(row['entity_sentiments'], dict):
            itemset.extend([f"entity={ent}" for ent in row['entity_sentiments'].keys()])
        
        transactions.append(itemset)
    
    # ä¸¥æ ¼æŒ‰è¯¾ç¨‹ç¤ºä¾‹è¿›è¡Œone-hotç¼–ç 
    te = TransactionEncoder()
    te_ary = te.fit(transactions).transform(transactions)
    return pd.DataFrame(te_ary, columns=te.columns_)

# ç”Ÿæˆäº‹åŠ¡æ•°æ®
trans_df = prepare_transaction_data(df)
print("äº‹åŠ¡æ•°æ®ç¤ºä¾‹ï¼ˆå‰5é¡¹ï¼‰ï¼š")
print(trans_df.iloc[0, :5])  # æ˜¾ç¤ºç¬¬ä¸€è¡Œçš„å‰5ä¸ªç‰¹å¾
```

ç»“æœ

äº‹åŠ¡æ•°æ®ç¤ºä¾‹ï¼ˆå‰5é¡¹ï¼‰ï¼š

category=BLACK VOICES     False

category=BUSINESS         False

category=COMEDY            True

category=ENTERTAINMENT    False

category=FOOD & DRINK     False

Name: 0, dtype: bool


```js
from mlxtend.frequent_patterns import apriori, association_rules

def mine_association_rules(trans_df):
    """
    æŒ‰è¯¾ç¨‹æ ‡å‡†å®ç°å…³è”è§„åˆ™æŒ–æ˜
    ä¿®æ”¹ç‚¹ï¼š
    1. ä½¿ç”¨è¯¾ç¨‹æŒ‡å®šçš„é»˜è®¤å‚æ•°
    2. ç®€åŒ–è§„åˆ™ç­›é€‰é€»è¾‘
    3. è¾“å‡ºæ ¼å¼ä¸è¯¾ç¨‹ç¤ºä¾‹ä¸€è‡´
    """
    # ä½¿ç”¨è¯¾ç¨‹æ¨èå‚æ•°ï¼ˆmin_support=0.1, min_threshold=0.7ï¼‰
    frequent_itemsets = apriori(trans_df, min_support=0.1, use_colnames=True)
    
    # ç”Ÿæˆè§„åˆ™ï¼ˆæŒ‰è¯¾ç¨‹è¦æ±‚ä½¿ç”¨confidenceä½œä¸ºåº¦é‡ï¼‰
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)
    
    # æŒ‰è¯¾ç¨‹ç¤ºä¾‹æ·»åŠ lengthåˆ—
    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))
    
    # ç­›é€‰æœ‰æ•ˆè§„åˆ™ï¼ˆlift>1ï¼‰
    meaningful_rules = rules[rules['lift'] > 1].copy()
    meaningful_rules['rule'] = meaningful_rules.apply(
        lambda row: f"{set(row['antecedents'])} â†’ {set(row['consequents'])}", 
        axis=1
    )
    
    return frequent_itemsets, meaningful_rules

# æ‰§è¡ŒæŒ–æ˜
frequent_itemsets, rules = mine_association_rules(trans_df)

# æ˜¾ç¤ºç»“æœï¼ˆæŒ‰è¯¾ç¨‹è¡¨æ ¼æ ¼å¼ï¼‰
print("\né¢‘ç¹é¡¹é›†ï¼ˆå‰5ä¸ªï¼‰ï¼š")
print(frequent_itemsets[['itemsets', 'support', 'length']].head())
print("\nå…³è”è§„åˆ™ï¼ˆå‰5æ¡ï¼‰ï¼š")
print(rules[['rule', 'support', 'confidence', 'lift']].head())
```
ç»“æœ

```js
é¢‘ç¹é¡¹é›†ï¼ˆå‰5ä¸ªï¼‰ï¼š
                   itemsets   support  length
0  (category=ENTERTAINMENT)  0.227578       1
1       (category=POLITICS)  0.316143       1
2      (sentiment=negative)  0.539238       1
3       (sentiment=neutral)  0.105381       1
4      (sentiment=positive)  0.355381       1

å…³è”è§„åˆ™ï¼ˆå‰5æ¡ï¼‰ï¼š
                                             rule   support  confidence  \
0  {'category=POLITICS'} â†’ {'sentiment=negative'}  0.232063    0.734043   

      lift  
0  1.36126
```


```js
import matplotlib.pyplot as plt

def plot_rules(rules):
    """
    æŒ‰è¯¾ç¨‹è¦æ±‚ç»˜åˆ¶è§„åˆ™çƒ­åŠ›å›¾
    ä¿®æ”¹ç‚¹ï¼š
    1. ä½¿ç”¨è¯¾ç¨‹æŒ‡å®šçš„å¯è§†åŒ–å½¢å¼
    2. ç®€åŒ–è§†è§‰å…ƒç´ 
    """
    if len(rules) == 0:
        print("æ— æœ‰æ•ˆè§„åˆ™å¯å¯è§†åŒ–")
        return
    
    plt.figure(figsize=(10, 6))
    plt.scatter(
        rules['support'],
        rules['confidence'],
        s=rules['lift']*50,
        c=rules['lift'],
        cmap='viridis',
        alpha=0.6
    )
    
    # æ·»åŠ è¯¾ç¨‹è¦æ±‚çš„æ ‡ç­¾
    plt.colorbar(label='Liftå€¼')
    plt.xlabel('Support', fontsize=12)
    plt.ylabel('Confidence', fontsize=12)
    plt.title('å…³è”è§„åˆ™çƒ­åŠ›å›¾ï¼ˆæŒ‰è¯¾ç¨‹è§„èŒƒï¼‰', fontsize=14)
    plt.grid(True, alpha=0.2)
    plt.tight_layout()
    plt.show()

# å¯è§†åŒ–
plot_rules(rules)
```

![image](https://github.com/user-attachments/assets/662e8a14-167b-45f8-8810-98191a2a6dd2)



```js
# 1. æ•°æ®å‡†å¤‡
print("=== æ­¥éª¤1ï¼šå‡†å¤‡äº‹åŠ¡æ•°æ® ===")
trans_df = prepare_transaction_data(df)

# 2. é¢‘ç¹é¡¹é›†æŒ–æ˜
print("\n=== æ­¥éª¤2ï¼šAprioriç®—æ³•æ‰§è¡Œ ===")
frequent_itemsets, rules = mine_association_rules(trans_df)

# 3. ç»“æœåˆ†æ
print("\n=== æ­¥éª¤3ï¼šè§„åˆ™åˆ†æ ===")
if len(rules) > 0:
    # æŒ‰è¯¾ç¨‹è¦æ±‚ä¿å­˜ç»“æœ
    rules.to_csv("association_rules.csv", index=False)
    print("å·²ä¿å­˜è§„åˆ™åˆ°association_rules.csv")
    
    # æƒ…æ„Ÿç›¸å…³è§„åˆ™åˆ†æï¼ˆè¯¾ç¨‹é¡¹ç›®è¦æ±‚ï¼‰
    sentiment_rules = rules[rules['consequents'].apply(
        lambda x: any('sentiment=' in item for item in x)
    )]
    print("\næƒ…æ„Ÿç›¸å…³è§„åˆ™ï¼ˆå‰5æ¡ï¼‰ï¼š")
    print(sentiment_rules[['rule', 'support', 'confidence', 'lift']].head())
else:
    print("æœªå‘ç°æ˜¾è‘—è§„åˆ™ï¼Œå»ºè®®é™ä½min_supportæˆ–min_threshold")

# 4. å¯è§†åŒ–
print("\n=== æ­¥éª¤4ï¼šå¯è§†åŒ– ===")
plot_rules(rules)
```

ç»“æœ

```js
=== æ­¥éª¤1ï¼šå‡†å¤‡äº‹åŠ¡æ•°æ® ===

=== æ­¥éª¤2ï¼šAprioriç®—æ³•æ‰§è¡Œ ===

=== æ­¥éª¤3ï¼šè§„åˆ™åˆ†æ ===
å·²ä¿å­˜è§„åˆ™åˆ°association_rules.csv

æƒ…æ„Ÿç›¸å…³è§„åˆ™ï¼ˆå‰5æ¡ï¼‰ï¼š
                                             rule   support  confidence  \
0  {'category=POLITICS'} â†’ {'sentiment=negative'}  0.232063    0.734043   

      lift  
0  1.36126  

=== æ­¥éª¤4ï¼šå¯è§†åŒ– ===
<ipython-input-39-fa4fe257c0ca>:30: UserWarning:

Glyph 20540 (\N{CJK UNIFIED IDEOGRAPH-503C}) missing from font(s) DejaVu Sans.

/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning:

Glyph 20540 (\N{CJK UNIFIED IDEOGRAPH-503C}) missing from font(s) DejaVu Sans.
```
![image](https://github.com/user-attachments/assets/58334a40-3889-40ab-ab04-5e80797cfe51)

```js
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules

# å‡†å¤‡å…³è”è§„åˆ™æŒ–æ˜æ‰€éœ€çš„æ•°æ®
def prepare_association_data(df):
    """
    å‡†å¤‡å…³è”è§„åˆ™æŒ–æ˜çš„æ•°æ®é›†
    è¿”å›:
        - å¤„ç†åçš„äº¤æ˜“æ•°æ®é›† (ç”¨äºaprioriç®—æ³•)
        - åŸå§‹DataFrameçš„æ‰©å±•ç‰ˆæœ¬ (åŒ…å«one-hotç¼–ç ç‰¹å¾)
    """
    # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
    if 'sentiment_label' not in df.columns:
        print("è¯·å…ˆè¿è¡Œæƒ…æ„Ÿåˆ†æ!")
        return None, None
    
    # 1. æå–æ¯è¡Œæ–‡æœ¬çš„ç‰¹å¾
    transactions = []
    for _, row in df.iterrows():
        features = []
        
        # æ·»åŠ æƒ…æ„Ÿæ ‡ç­¾
        features.append(f"sentiment={row['sentiment_label']}")
        
        # æ·»åŠ æ–°é—»ç±»åˆ« (å¦‚æœå­˜åœ¨)
        if 'category' in df.columns:
            features.append(f"category={row['category']}")
        
        # æ·»åŠ å…¥é€‰å®ä½“ (å¦‚æœå­˜åœ¨)
        if 'entity_sentiments' in df.columns and isinstance(row['entity_sentiments'], dict):
            for entity in row['entity_sentiments'].keys():
                features.append(f"entity={entity}")
        
        # æ·»åŠ é«˜é¢‘å…³é”®è¯ (ä»sentiment_textæå–)
        if 'sentiment_text' in df.columns:
            top_words = [word for word in str(row['sentiment_text']).split() 
                        if len(word) > 3][:5]  # å–é•¿åº¦>3çš„å‰5ä¸ªè¯
            for word in top_words:
                features.append(f"word={word.lower()}")
        
        transactions.append(features)
    
    # 2. è½¬æ¢ä¸ºone-hotç¼–ç æ ¼å¼
    te = TransactionEncoder()
    te_ary = te.fit(transactions).transform(transactions)
    trans_df = pd.DataFrame(te_ary, columns=te.columns_)
    
    return transactions, trans_df

# å‡†å¤‡æ•°æ®
transactions, trans_df = prepare_association_data(df)
print(f"ç”Ÿæˆ {len(trans_df.columns)} ä¸ªç‰¹å¾:")
print(trans_df.columns.tolist()[:10])  # æ˜¾ç¤ºå‰10ä¸ªç‰¹å¾
```
ç»“æœ

```js
ç”Ÿæˆ 2813 ä¸ªç‰¹å¾:
['category=BLACK VOICES', 'category=BUSINESS', 'category=COMEDY', 'category=ENTERTAINMENT', 'category=FOOD & DRINK', 'category=HEALTHY LIVING', 'category=HOME & LIVING', 'category=PARENTING', 'category=PARENTS', 'category=POLITICS']
```

```js
from mlxtend.frequent_patterns import apriori

def run_apriori(trans_df, min_support=0.05):
    """
    ä¸¥æ ¼æŒ‰è¯¾ç¨‹ä¼ªä»£ç å®ç°Aprioriç®—æ³•
    å‚æ•°:
        min_support: å®Œå…¨æŒ‰è¯¾ç¨‹å®šä¹‰çš„æ”¯æŒåº¦é˜ˆå€¼
    è¿”å›:
        - é¢‘ç¹é¡¹é›†DataFrame (å«supportåˆ—)
    """
    # ç›´æ¥è°ƒç”¨mlxtendå®ç° (ä¸è¯¾ç¨‹ä¼ªä»£ç é€»è¾‘ä¸€è‡´)
    frequent_itemsets = apriori(
        trans_df, 
        min_support=min_support,
        use_colnames=True,  # ä¿ç•™åŸå§‹é¡¹é›†åç§°
        max_len=None        # ä¸é™åˆ¶é¡¹é›†é•¿åº¦
    )
    
    # æŒ‰è¯¾ç¨‹è¦æ±‚æ·»åŠ lengthåˆ—
    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))
    
    print(f"\nç”Ÿæˆçš„é¢‘ç¹é¡¹é›† (min_support={min_support}):")
    print(frequent_itemsets.sort_values('support', ascending=False).head())
    return frequent_itemsets

# æ‰§è¡ŒAprioriç®—æ³• (å‚æ•°ä¸è¯¾ç¨‹ç¤ºä¾‹ä¸€è‡´)
frequent_itemsets = run_apriori(trans_df, min_support=0.03)
```

ç»“æœÂ·

```js

ç”Ÿæˆçš„é¢‘ç¹é¡¹é›† (min_support=0.03):
     support                                 itemsets  length
10  0.539238                     (sentiment=negative)       1
12  0.355381                     (sentiment=positive)       1
5   0.316143                      (category=POLITICS)       1
18  0.232063  (sentiment=negative, category=POLITICS)       2
1   0.227578                 (category=ENTERTAINMENT)       1
```



```js
from mlxtend.frequent_patterns import association_rules

def generate_rules(frequent_itemsets, min_confidence=0.7):
    """
    ä¸¥æ ¼æŒ‰è¯¾ç¨‹å®šä¹‰çš„æŒ‡æ ‡ç”Ÿæˆè§„åˆ™:
    - support: P(X âˆª Y)
    - confidence: P(Y|X) = support(X âˆª Y) / support(X)
    - lift: P(X âˆª Y) / (P(X)P(Y))
    """
    rules = association_rules(
        frequent_itemsets,
        metric="confidence",
        min_threshold=min_confidence
    )
    
    # æŒ‰è¯¾ç¨‹è¦æ±‚è®¡ç®—lift (å·²è‡ªåŠ¨è®¡ç®—)
    rules = rules.sort_values(['lift', 'confidence'], ascending=False)
    
    print(f"\nç”Ÿæˆçš„å…³è”è§„åˆ™ (min_confidence={min_confidence}):")
    print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head())
    return rules

# ç”Ÿæˆè§„åˆ™ (å‚æ•°ä¸è¯¾ç¨‹ç¤ºä¾‹ä¸€è‡´)
rules = generate_rules(frequent_itemsets, min_confidence=0.5)
```
ç»“æœ

```js
ç”Ÿæˆçš„å…³è”è§„åˆ™ (min_confidence=0.5):
                        antecedents                              consequents  \
1                      (entity=gop)                      (category=POLITICS)   
6  (sentiment=negative, word=trump)                      (category=POLITICS)   
3                      (word=trump)                      (category=POLITICS)   
8                      (word=trump)  (sentiment=negative, category=POLITICS)   
0           (category=FOOD & DRINK)                     (sentiment=positive)   

    support  confidence      lift  
1  0.034753    0.968750  3.064273  
6  0.036996    0.891892  2.821162  
3  0.052691    0.886792  2.805031  
8  0.036996    0.622642  2.683074  
0  0.033632    0.638298  1.796094
```


```js
def analyze_rules(rules):
    """æŒ‰è¯¾ç¨‹è¦æ±‚åˆ†æè§„åˆ™"""
    # 1. ç­›é€‰æœ‰æ•ˆè§„åˆ™ (lift > 1è¡¨ç¤ºæ­£ç›¸å…³)
    meaningful_rules = rules[rules['lift'] > 1].copy()
    
    # 2. æŒ‰è¯¾ç¨‹ç¤ºä¾‹æ·»åŠ è§„åˆ™é•¿åº¦
    meaningful_rules['antecedent_len'] = meaningful_rules['antecedents'].apply(lambda x: len(x))
    meaningful_rules['consequent_len'] = meaningful_rules['consequents'].apply(lambda x: len(x))
    
    # 3. æŒ‰è¯¾ç¨‹åˆ†ç±»å±•ç¤º
    print("\n=== æ­£ç›¸å…³è§„åˆ™ (lift > 1) ===")
    print(meaningful_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head())
    
    # 4. æƒ…æ„Ÿç›¸å…³è§„åˆ™ (æŒ‰è¯¾ç¨‹é¡¹ç›®è¦æ±‚)
    sentiment_rules = meaningful_rules[
        meaningful_rules['consequents'].apply(
            lambda x: any('sentiment=' in item for item in x)
        )
    ]
    
    print("\n=== æƒ…æ„Ÿå…³è”è§„åˆ™ ===")
    print(sentiment_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head())

# æ‰§è¡Œåˆ†æ
analyze_rules(rules)
```



ç»“æœÂ·

```js
=== æ­£ç›¸å…³è§„åˆ™ (lift > 1) ===
                        antecedents                              consequents  \
1                      (entity=gop)                      (category=POLITICS)   
6  (sentiment=negative, word=trump)                      (category=POLITICS)   
3                      (word=trump)                      (category=POLITICS)   
8                      (word=trump)  (sentiment=negative, category=POLITICS)   
0           (category=FOOD & DRINK)                     (sentiment=positive)   

    support  confidence      lift  
1  0.034753    0.968750  3.064273  
6  0.036996    0.891892  2.821162  
3  0.052691    0.886792  2.805031  
8  0.036996    0.622642  2.683074  
0  0.033632    0.638298  1.796094  

=== æƒ…æ„Ÿå…³è”è§„åˆ™ ===
                       antecedents                              consequents  \
8                     (word=trump)  (sentiment=negative, category=POLITICS)   
0          (category=FOOD & DRINK)                     (sentiment=positive)   
2              (category=POLITICS)                     (sentiment=negative)   
7  (category=POLITICS, word=trump)                     (sentiment=negative)   
5                     (word=trump)                     (sentiment=negative)   

    support  confidence      lift  
8  0.036996    0.622642  2.683074  
0  0.033632    0.638298  1.796094  
2  0.232063    0.734043  1.361260  
7  0.036996    0.702128  1.302075  
5  0.041480    0.698113  1.294630
```


```js
import matplotlib.pyplot as plt

def plot_rules(rules, top_n=10):
    """æŒ‰è¯¾ç¨‹é¡¹ç›®ç¤ºä¾‹ç»˜åˆ¶è§„åˆ™çƒ­åŠ›å›¾"""
    if len(rules) == 0:
        print("æ— æœ‰æ•ˆè§„åˆ™å¯å¯è§†åŒ–")
        return
    
    # å‡†å¤‡æ•°æ® (æŒ‰è¯¾ç¨‹ç¤ºä¾‹æ ¼å¼)
    plot_data = rules.head(top_n).copy()
    plot_data['rule'] = plot_data.apply(
        lambda row: f"{set(row['antecedents'])} â†’ {set(row['consequents'])}",
        axis=1
    )
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾ (å®Œå…¨æŒ‰è¯¾ç¨‹é£æ ¼)
    plt.figure(figsize=(10, 6))
    plt.scatter(
        plot_data['support'],
        plot_data['confidence'],
        s=plot_data['lift'] * 100,  # ç‚¹å¤§å°è¡¨ç¤ºlift
        c=plot_data['lift'],        # é¢œè‰²è¡¨ç¤ºlift
        alpha=0.6,
        cmap='viridis'
    )
    
    # æ·»åŠ æ ‡ç­¾ (æŒ‰è¯¾ç¨‹ç¤ºä¾‹)
    for i, row in plot_data.iterrows():
        plt.annotate(
            row['rule'],
            (row['support'], row['confidence']),
            xytext=(5, 5),
            textcoords='offset points',
            fontsize=8
        )
    
    plt.colorbar(label='Lift')
    plt.xlabel('Support (P(XâˆªY))', fontsize=12)
    plt.ylabel('Confidence (P(Y|X))', fontsize=12)
    plt.title('Association rule heat map (size/color indicates lift)', fontsize=14)
    plt.grid(True, alpha=0.2)
    plt.tight_layout()
    plt.show()

# å¯è§†åŒ–
plot_rules(rules[rules['lift'] > 1])
```

![image](https://github.com/user-attachments/assets/cf1618ef-e651-4f9e-8031-44a23397ae49)


```js
def mine_association_rules(trans_df, min_support=0.05, min_threshold=0.5):
    """
    æ‰§è¡Œå…³è”è§„åˆ™æŒ–æ˜
    å‚æ•°:
        trans_df: one-hotç¼–ç çš„äº¤æ˜“æ•°æ®
        min_support: æœ€å°æ”¯æŒåº¦
        min_threshold: æœ€å°æå‡åº¦é˜ˆå€¼
    """
    # 1. æ‰¾å‡ºé¢‘ç¹é¡¹é›†
    frequent_itemsets = apriori(trans_df, min_support=min_support, use_colnames=True)
    print(f"\nFound {len(frequent_itemsets)} frequent itemsets (min_support={min_support})")
    
    if len(frequent_itemsets) == 0:
        print("æ²¡æœ‰æ‰¾åˆ°é¢‘ç¹é¡¹é›†ï¼Œè¯·é™ä½min_supporté˜ˆå€¼")
        return None
    
    # 2. ç”Ÿæˆå…³è”è§„åˆ™
    rules = association_rules(frequent_itemsets, metric="lift", min_threshold=min_threshold)
    print(f"Generated {len(rules)} association rules (min_threshold={min_threshold})")
    
    if len(rules) == 0:
        print("æ²¡æœ‰ç”Ÿæˆå…³è”è§„åˆ™ï¼Œè¯·é™ä½min_thresholdé˜ˆå€¼")
        return None
    
    # 3. è¿‡æ»¤å¹¶æ’åºè§„åˆ™
    interesting_rules = rules[
        (rules['consequents'].apply(lambda x: any('sentiment=' in item for item in x))) &
        (rules['lift'] > 1)
    ].sort_values(['lift', 'confidence'], ascending=False)
    
    return interesting_rules

# æ‰§è¡Œå…³è”è§„åˆ™æŒ–æ˜
rules = mine_association_rules(trans_df, min_support=0.03, min_threshold=0.7)

# æ˜¾ç¤ºæœ€æœ‰æ„ä¹‰çš„è§„åˆ™
if rules is not None and len(rules) > 0:
    pd.set_option('display.max_colwidth', 100)
    print("\nTop 10 Interesting association rules:")
    display(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))
else:
    print("\næ²¡æœ‰å‘ç°æœ‰è¶£çš„å…³è”è§„åˆ™")
```


![image](https://github.com/user-attachments/assets/954de244-95f3-4560-be82-0e623bb5286a)

 
```js
import networkx as nx
import matplotlib.pyplot as plt

def visualize_rules(rules, top_n=10):
    """å¯è§†åŒ–å…³è”è§„åˆ™ç½‘ç»œå›¾"""
    if rules is None or len(rules) == 0:
        print("æ²¡æœ‰å¯å¯è§†åŒ–çš„è§„åˆ™")
        return
    
    # å‡†å¤‡æ•°æ®
    top_rules = rules.head(top_n)
    graph = nx.DiGraph()
    
    # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
    for _, row in top_rules.iterrows():
        antecedents = ', '.join(list(row['antecedents']))
        consequents = ', '.join(list(row['consequents']))
        weight = row['lift']
        
        graph.add_edge(antecedents, consequents, weight=weight)
    
    # ç»˜åˆ¶ç½‘ç»œå›¾
    plt.figure(figsize=(12, 8))
    pos = nx.spring_layout(graph, k=0.5)
    
    # èŠ‚ç‚¹å¤§å°åŸºäºåº¦æ•°
    node_sizes = [3000 * graph.degree(node) for node in graph.nodes()]
    
    # è¾¹å®½åº¦åŸºäºliftå€¼
    edge_widths = [2 * row['weight'] for _, _, row in graph.edges(data=True)]
    
    # ç»˜åˆ¶
    nx.draw_networkx_nodes(graph, pos, node_size=node_sizes, alpha=0.7,
                          node_color='skyblue')
    nx.draw_networkx_edges(graph, pos, width=edge_widths, alpha=0.5,
                          edge_color='gray', arrowsize=20)
    nx.draw_networkx_labels(graph, pos, font_size=10, font_weight='bold')
    
    # æ·»åŠ æ ‡é¢˜å’Œè¯´æ˜
    plt.title(f"Top {top_n} Sentiment association rules (edge â€‹â€‹width represents lift value)", fontsize=14)
    plt.axis('off')
    
    # æ·»åŠ å›¾ä¾‹
    plt.text(0.5, -0.1, 
             "Arrow direction: Precondition â†’ Result\nNode size: Number of connections\nEdge width: Rule lift value",
             ha='center', transform=plt.gca().transAxes)
    
    plt.tight_layout()
    plt.show()

# å¯è§†åŒ–è§„åˆ™
visualize_rules(rules)

# åˆ†æç‰¹å®šæƒ…æ„Ÿç±»åˆ«çš„è§„åˆ™
if rules is not None:
    for sentiment in ['positive', 'negative', 'neutral']:
        sent_rules = rules[rules['consequents'].apply(
            lambda x: f"sentiment={sentiment}" in x)]
        
        if len(sent_rules) > 0:
            print(f"\nä¸'{sentiment}'æƒ…æ„Ÿç›¸å…³çš„è§„åˆ™:")
            display(sent_rules[['antecedents', 'consequents', 
                              'support', 'confidence', 'lift']].head(5))
```

![image](https://github.com/user-attachments/assets/d4a0f2ce-6c56-42e2-893a-b7dacf30b50d)

![image](https://github.com/user-attachments/assets/f7e7d16b-57a7-44f6-9188-8d7f8069abd3)

```js
# 1. æ•°æ®å‡†å¤‡
print("æ­¥éª¤1: å‡†å¤‡å…³è”è§„åˆ™æŒ–æ˜æ•°æ®...")
transactions, trans_df = prepare_association_data(df)

# 2. æŒ–æ˜å…³è”è§„åˆ™
print("\næ­¥éª¤2: æŒ–æ˜å…³è”è§„åˆ™...")
rules = mine_association_rules(trans_df, 
                              min_support=0.03, 
                              min_threshold=0.7)

# 3. å¯è§†åŒ–ä¸åˆ†æ
if rules is not None and len(rules) > 0:
    print("\næ­¥éª¤3: å¯è§†åŒ–ä¸åˆ†æ...")
    visualize_rules(rules)
    
    # ä¿å­˜ç»“æœ
    rules.to_csv("sentiment_association_rules.csv", index=False)
    print("\nå…³è”è§„åˆ™å·²ä¿å­˜åˆ° sentiment_association_rules.csv")
else:
    print("\næ²¡æœ‰å‘ç°æ˜¾è‘—çš„å…³è”è§„åˆ™")

```

ç»“æœ

```js
æ­¥éª¤1: å‡†å¤‡å…³è”è§„åˆ™æŒ–æ˜æ•°æ®...

æ­¥éª¤2: æŒ–æ˜å…³è”è§„åˆ™...

Found 25 frequent itemsets (min_support=0.03)
Generated 24 association rules (min_threshold=0.7)

æ­¥éª¤3: å¯è§†åŒ–ä¸åˆ†æ...
```
![image](https://github.com/user-attachments/assets/82de8446-6b9e-4c69-8a18-fc08a4a3ad3d)

å…³è”è§„åˆ™å·²ä¿å­˜åˆ° sentiment_association_rules.csv
# åŸºäºæƒ…æ„Ÿï¼Œå®ä½“å’Œå†…å®¹çš„æ¨èç³»ç»Ÿ

# åŸºäºå…³è”è§„åˆ™çš„æŒ–æ˜ç³»ç»Ÿ

