# 完整实体提取与主题分析实现

我将为您实现缺失的实体提取(NER)和主题建模(LDA)部分，使整个关联规则挖掘流程能够完整运行。

## 1. 命名实体识别(NER)实现

```python
import spacy
from collections import defaultdict

# 加载英文模型
nlp = spacy.load("en_core_web_sm")

def extract_entities(text):
    """
    使用spacy进行命名实体识别
    返回格式: [(实体文本, 实体类型), ...]
    """
    doc = nlp(text)
    entities = []
    
    # 提取有意义的实体类型
    allowed_types = {'PERSON', 'ORG', 'GPE', 'PRODUCT', 'EVENT'}
    
    for ent in doc.ents:
        if ent.label_ in allowed_types:
            # 标准化实体文本(小写,去空格)
            normalized_text = ent.text.lower().strip()
            if len(normalized_text) > 2:  # 过滤过短实体
                entities.append((normalized_text, ent.label_))
    
    return entities

# 应用到整个数据集
def add_entity_column(df):
    """为DataFrame添加实体列"""
    print("开始实体提取...")
    df['entities'] = df['text'].progress_apply(extract_entities)  # 使用progress_apply需要tqdm
    print("实体提取完成!")
    return df

# 示例使用
# df = add_entity_column(df)
```

## 2. 主题建模(LDA)实现

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np

def train_lda_model(df, n_topics=5, max_features=1000):
    """
    训练LDA主题模型并返回主题分布
    """
    # 1. 文本向量化
    vectorizer = CountVectorizer(max_df=0.95, min_df=2,
                               max_features=max_features,
                               stop_words='english')
    dtm = vectorizer.fit_transform(df['cleaned_text'])
    
    # 2. 训练LDA模型
    lda = LatentDirichletAllocation(n_components=n_topics,
                                  random_state=42,
                                  learning_method='online')
    lda.fit(dtm)
    
    # 3. 获取每个文档的主题分布
    topic_distributions = lda.transform(dtm)
    
    # 4. 获取每个文档的主主题
    df['dominant_topic'] = np.argmax(topic_distributions, axis=1)
    
    # 5. 获取主题关键词
    feature_names = vectorizer.get_feature_names_out()
    topic_keywords = []
    
    for topic_idx, topic in enumerate(lda.components_):
        top_keywords_idx = topic.argsort()[:-10 - 1:-1]
        top_keywords = [feature_names[i] for i in top_keywords_idx]
        topic_keywords.append(top_keywords)
    
    return df, topic_keywords

def assign_topics_to_articles(df, topic_keywords):
    """
    为每篇文章分配主题标签
    """
    # 将主题关键词转换为标签
    topic_labels = []
    for i, keywords in enumerate(topic_keywords):
        label = f"topic_{i}_" + "_".join(keywords[:3])
        topic_labels.append(label)
    
    # 为每篇文章分配主题标签
    df['topics'] = df['dominant_topic'].apply(lambda x: [topic_labels[x]])
    
    return df

# 完整主题分析流程
def perform_topic_analysis(df):
    """执行完整的主题分析流程"""
    print("开始主题建模...")
    df, topic_keywords = train_lda_model(df)
    df = assign_topics_to_articles(df, topic_keywords)
    print("主题建模完成!")
    print("\n主题关键词:")
    for i, keywords in enumerate(topic_keywords):
        print(f"主题{i}: {', '.join(keywords[:5])}...")
    return df
```

## 3. 情感分析实现

```python
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def add_sentiment_scores(df):
    """添加情感分析列"""
    analyzer = SentimentIntensityAnalyzer()
    
    print("开始情感分析...")
    df['sentiment'] = df['text'].progress_apply(
        lambda x: analyzer.polarity_scores(x)['compound']
    )
    print("情感分析完成!")
    return df
```

## 4. 完整数据处理流程

```python
from tqdm import tqdm
tqdm.pandas()  # 启用pandas的进度条

def full_data_processing_pipeline(df):
    """完整的数据处理流程"""
    # 1. 实体识别
    df = add_entity_column(df)
    
    # 2. 主题分析
    df = perform_topic_analysis(df)
    
    # 3. 情感分析
    df = add_sentiment_scores(df)
    
    return df

# 使用示例
# df = pd.read_csv('preprocessed_news.csv')
# df = full_data_processing_pipeline(df)
```

## 5. 关联规则挖掘的完整实现

现在我们可以完整实现之前缺失的两部分：

```python
def prepare_entity_transactions(df):
    """准备实体事务数据"""
    transactions = []
    for entities in df['entities']:  # 现在df['entities']已经存在
        unique_entities = list(set([e[0] for e in entities]))  # 去重
        transactions.append(unique_entities)
    return transactions

def prepare_sentiment_transactions(df):
    """准备情感-主题事务数据"""
    transactions = []
    for _, row in df.iterrows():
        topics = row['topics']  # 现在df['topics']已经存在
        sentiment = 'positive' if row['sentiment'] > 0 else 'negative'
        transaction = topics + [f"sentiment_{sentiment}"]
        transactions.append(transaction)
    return transactions

# 完整关联规则挖掘示例
def full_association_mining(df):
    # 1. 实体关联规则
    entity_transactions = prepare_entity_transactions(df)
    entity_rules = mine_association_rules(entity_transactions, 
                                        min_support=0.02, 
                                        min_confidence=0.6)
    
    # 2. 情感-主题关联规则
    sentiment_transactions = prepare_sentiment_transactions(df)
    sentiment_rules = mine_association_rules(sentiment_transactions,
                                           min_support=0.1,
                                           min_confidence=0.7)
    
    return entity_rules, sentiment_rules

def mine_association_rules(transactions, min_support, min_confidence):
    """通用的关联规则挖掘函数"""
    from mlxtend.preprocessing import TransactionEncoder
    from mlxtend.frequent_patterns import apriori, association_rules
    
    te = TransactionEncoder()
    te_ary = te.fit_transform(transactions)
    df = pd.DataFrame(te_ary, columns=te.columns_)
    
    frequent_itemsets = apriori(df, min_support=min_support, 
                              use_colnames=True, max_len=4)
    
    rules = association_rules(frequent_itemsets, metric="confidence",
                            min_threshold=min_confidence)
    
    # 过滤提升度>1的有意义规则
    rules = rules[rules['lift'] > 1].sort_values('lift', ascending=False)
    
    return rules
```

## 6. 可视化与结果分析

```python
def visualize_top_rules(rules, title, top_n=10):
    """可视化前N条关联规则"""
    import matplotlib.pyplot as plt
    
    top_rules = rules.head(top_n).copy()
    top_rules['rule'] = top_rules['antecedents'].astype(str) + " → " + \
                       top_rules['consequents'].astype(str)
    
    plt.figure(figsize=(10, 6))
    plt.barh(top_rules['rule'], top_rules['lift'], color='skyblue')
    plt.xlabel('Lift Score')
    plt.title(f'Top {top_n} {title} Association Rules by Lift')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

# 示例使用
# entity_rules, sentiment_rules = full_association_mining(df)
# visualize_top_rules(entity_rules, "Entity")
# visualize_top_rules(sentiment_rules, "Sentiment-Topic")
```

## 7. 完整工作流程示例

```python
# 假设我们已经有了预处理好的DataFrame
# df = pd.read_csv('preprocessed_news.csv')

# 1. 执行完整的数据增强处理
df = full_data_processing_pipeline(df)

# 2. 挖掘关联规则
entity_rules, sentiment_rules = full_association_mining(df)

# 3. 查看结果
print("\n实体关联规则示例:")
print(entity_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head())

print("\n情感-主题关联规则示例:")
print(sentiment_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head())

# 4. 可视化
visualize_top_rules(entity_rules, "Entity")
visualize_top_rules(sentiment_rules, "Sentiment-Topic")
```

## 关键实现说明

1. **实体提取**：
   - 使用spacy的预训练模型识别5类关键实体：人物、组织、地点、产品和事件
   - 对实体文本进行标准化处理(小写、去空格)
   - 过滤过短的实体(长度>2)

2. **主题建模**：
   - 使用LDA算法发现新闻中的隐藏主题
   - 每个文档分配一个主主题
   - 为主题生成可解释的标签(基于关键词)

3. **情感分析**：
   - 使用VADER情感分析器计算复合情感分数
   - 将情感分为positive/negative两类

4. **关联规则挖掘**：
   - 实现两种类型的规则挖掘：
     - 实体共现规则(如"苹果"和"iPhone"经常一起出现)
     - 情感-主题关联规则(如"政治话题"常伴随"负面情绪")
   - 使用lift指标筛选有意义的规则

这个实现完整覆盖了您需要的所有功能，并提供了可视化工具来分析结果。所有步骤都有清晰的流程和进度提示，适合处理大规模新闻数据集。
