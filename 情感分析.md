# 情感分析实现方案

基于您提供的情感分析课程框架和HuffPost新闻数据集，我将实现一个完整的情感分析流程，包括实体提取、方面提取和情感分类。

## 1. 数据准备与预处理

首先加载预处理好的数据，并添加情感分析所需的额外处理：

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from textblob import TextBlob
import spacy
from spacy import displacy

# 加载预处理好的数据
df = pd.read_csv('preprocessed_news.csv')

# 加载spacy英文模型
nlp = spacy.load('en_core_web_sm')

# 添加情感分析专用预处理
def sentiment_preprocess(text):
    # 保留否定词和程度副词
    text = re.sub(r'[^a-zA-Z\s]', '', str(text).lower())
    doc = nlp(text)
    # 保留情感相关词性: 形容词、副词、动词、否定词
    tokens = [token.text for token in doc if token.pos_ in ('ADJ', 'ADV', 'VERB') or 
              token.dep_ == 'neg']
    return ' '.join(tokens)

df['sentiment_text'] = df['text'].apply(sentiment_preprocess)
```

## 2. 实体与方面提取

实现课程中提到的实体提取和方面提取功能：

```python
def extract_entities_and_aspects(text):
    doc = nlp(text)
    entities = []
    aspects = []
    
    # 提取命名实体(产品、组织、地点等)
    for ent in doc.ents:
        if ent.label_ in ('ORG', 'PRODUCT', 'PERSON', 'GPE'):
            entities.append((ent.text, ent.label_))
    
    # 提取方面(名词短语)
    for chunk in doc.noun_chunks:
        # 过滤通用名词
        if chunk.root.pos_ == 'NOUN' and len(chunk.text) > 3:
            aspects.append(chunk.text)
    
    return list(set(entities)), list(set(aspects))

# 示例应用
sample_text = "The new iPhone's battery life is disappointing compared to Samsung's latest model."
entities, aspects = extract_entities_and_aspects(sample_text)
print("Entities:", entities)
print("Aspects:", aspects)

# 应用到整个数据集(抽样部分数据)
sample_df = df.sample(1000, random_state=42)
sample_df[['entities', 'aspects']] = sample_df['text'].apply(
    lambda x: pd.Series(extract_entities_and_aspects(x))
)
```

## 3. 情感分类实现

结合课程中提到的三种方法实现情感分类：

### 3.1 基于规则的情感分析

```python
# 情感词典方法
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

def rule_based_sentiment(text):
    # VADER适合社交媒体和新闻文本
    scores = analyzer.polarity_scores(text)
    if scores['compound'] >= 0.05:
        return 'positive'
    elif scores['compound'] <= -0.05:
        return 'negative'
    else:
        return 'neutral'

# 应用规则方法
sample_df['rule_sentiment'] = sample_df['text'].apply(rule_based_sentiment)
```

### 3.2 基于统计分类的情感分析

```python
# 监督学习方法
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# 创建标注数据(这里用规则方法生成伪标签，实际应用应使用真实标注)
sample_df['label'] = sample_df['text'].apply(rule_based_sentiment)

# 特征工程
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(sample_df['sentiment_text'])
y = sample_df['label']

# 划分训练测试集
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 训练朴素贝叶斯分类器
nb_clf = MultinomialNB()
nb_clf.fit(X_train, y_train)

# 评估
y_pred = nb_clf.predict(X_test)
print(classification_report(y_test, y_pred))

# 应用到整个数据集
sample_df['ml_sentiment'] = nb_clf.predict(X)
```

### 3.3 基于方面的情感分析

```python
# 方面级情感分析
def aspect_based_sentiment(text):
    doc = nlp(text)
    aspects = {}
    
    # 提取方面和修饰词
    for token in doc:
        if token.pos_ == 'NOUN' and len(token.text) > 3:
            aspect = token.text
            # 查找修饰该名词的形容词
            for child in token.children:
                if child.pos_ == 'ADJ':
                    sentiment = analyzer.polarity_scores(child.text)['compound']
                    aspects[aspect] = {
                        'adjective': child.text,
                        'sentiment': 'positive' if sentiment > 0 else 'negative' if sentiment < 0 else 'neutral'
                    }
    return aspects

# 示例
aspect_results = aspect_based_sentiment(
    "The camera quality is excellent but battery life is poor."
)
print(aspect_results)

# 应用到部分数据
sample_df['aspect_sentiment'] = sample_df['text'].apply(aspect_based_sentiment)
```

## 4. 可视化与分析

```python
# 1. 整体情感分布
plt.figure(figsize=(10, 6))
sns.countplot(data=sample_df, x='rule_sentiment', order=['positive', 'neutral', 'negative'])
plt.title('Overall Sentiment Distribution (Rule-based)')
plt.show()

# 2. 按类别的情绪分布
plt.figure(figsize=(12, 8))
sns.countplot(data=sample_df, x='category', hue='rule_sentiment', 
             hue_order=['positive', 'neutral', 'negative'])
plt.title('Sentiment Distribution by Category')
plt.xticks(rotation=45)
plt.legend(title='Sentiment')
plt.tight_layout()
plt.show()

# 3. 情感词云
def generate_sentiment_wordcloud(df, sentiment_type):
    text = ' '.join(df[df['rule_sentiment'] == sentiment_type]['sentiment_text'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud)
    plt.title(f'{sentiment_type.capitalize()} Sentiment Word Cloud')
    plt.axis('off')
    plt.show()

generate_sentiment_wordcloud(sample_df, 'positive')
generate_sentiment_wordcloud(sample_df, 'negative')

# 4. 高频实体和方面的情感分析
# 提取所有实体
all_entities = [item for sublist in sample_df['entities'].tolist() for item in sublist]
entity_df = pd.DataFrame(all_entities, columns=['entity', 'type'])

# 统计高频实体
top_entities = entity_df['entity'].value_counts().head(10).index

# 分析高频实体的情感倾向
entity_sentiment = []
for entity in top_entities:
    # 获取包含该实体的所有文本
    texts = sample_df[sample_df['text'].str.contains(entity, case=False)]['text']
    if len(texts) > 0:
        avg_sentiment = np.mean([analyzer.polarity_scores(text)['compound'] for text in texts])
        entity_sentiment.append((entity, avg_sentiment))

# 可视化
entity_sentiment_df = pd.DataFrame(entity_sentiment, columns=['entity', 'sentiment'])
plt.figure(figsize=(12, 6))
sns.barplot(data=entity_sentiment_df, x='sentiment', y='entity', palette='coolwarm')
plt.title('Sentiment Analysis of Top Entities')
plt.xlabel('Average Sentiment Score')
plt.ylabel('Entity')
plt.axvline(x=0, color='k', linestyle='--')
plt.tight_layout()
plt.show()
```

## 5. 商业应用场景实现

### 5.1 消费者洞察分析

```python
# 分析特定类别(如TECH)的负面评价关键词
tech_df = sample_df[sample_df['category'] == 'TECH']
negative_tech = tech_df[tech_df['rule_sentiment'] == 'negative']

# 提取负面高频词
all_words = ' '.join(negative_tech['sentiment_text']).split()
word_counts = Counter(all_words)
top_negative_words = word_counts.most_common(20)

# 可视化
plt.figure(figsize=(12, 6))
sns.barplot(x=[count for word, count in top_negative_words], 
            y=[word for word, count in top_negative_words])
plt.title('Top Negative Words in TECH Category')
plt.xlabel('Frequency')
plt.ylabel('Word')
plt.tight_layout()
plt.show()
```

### 5.2 竞争分析

```python
# 比较不同实体(如Apple vs Google)的情感分布
def compare_entity_sentiment(entity1, entity2):
    entity1_texts = sample_df[sample_df['text'].str.contains(entity1, case=False)]
    entity2_texts = sample_df[sample_df['text'].str.contains(entity2, case=False)]
    
    # 计算情感分布
    entity1_sentiment = entity1_texts['rule_sentiment'].value_counts(normalize=True)
    entity2_sentiment = entity2_texts['rule_sentiment'].value_counts(normalize=True)
    
    # 可视化
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    entity1_sentiment.plot(kind='bar', ax=ax1, color=['green', 'gray', 'red'])
    ax1.set_title(f'Sentiment Distribution for {entity1}')
    ax1.set_ylabel('Percentage')
    
    entity2_sentiment.plot(kind='bar', ax=ax2, color=['green', 'gray', 'red'])
    ax2.set_title(f'Sentiment Distribution for {entity2}')
    
    plt.tight_layout()
    plt.show()

compare_entity_sentiment('Apple', 'Google')
```

### 5.3 舆情动态监控

```python
# 时间序列情感分析
sample_df['date'] = pd.to_datetime(sample_df['date'])
sample_df['month'] = sample_df['date'].dt.to_period('M')

# 按月计算平均情感得分
def get_sentiment_score(text):
    return analyzer.polarity_scores(text)['compound']

sample_df['sentiment_score'] = sample_df['text'].apply(get_sentiment_score)
monthly_sentiment = sample_df.groupby('month')['sentiment_score'].mean()

# 可视化
plt.figure(figsize=(14, 6))
monthly_sentiment.plot(kind='line', marker='o')
plt.axhline(y=0, color='k', linestyle='--')
plt.title('Monthly Average Sentiment Score Over Time')
plt.ylabel('Average Sentiment Score')
plt.xlabel('Month')
plt.grid(True)
plt.tight_layout()
plt.show()

# 负面舆情预警
negative_spikes = monthly_sentiment[monthly_sentiment < -0.1]
if not negative_spikes.empty:
    print("Negative sentiment spikes detected in:")
    for month, score in negative_spikes.items():
        print(f"- {month}: Score = {score:.2f}")
        # 可以进一步分析该月的负面新闻主题
        month_negatives = sample_df[(sample_df['month'] == month) & 
                                  (sample_df['sentiment_score'] < -0.5)]
        if not month_negatives.empty:
            print("  Top negative headlines:")
            for idx, row in month_negatives.head(3).iterrows():
                print(f"  - {row['headline']} (Score: {row['sentiment_score']:.2f})")
```

## 6. 技术挑战处理

针对课程提到的技术挑战，实现以下解决方案：

### 6.1 讽刺检测

```python
# 简单的讽刺检测(基于模式匹配)
def detect_sarcasm(text):
    # 检查是否存在正面词+负面上下文或反之
    positive_words = ['great', 'excellent', 'wonderful', 'love', 'best']
    negative_words = ['terrible', 'awful', 'horrible', 'worst', 'hate']
    
    doc = nlp(text.lower())
    has_positive = any(token.text in positive_words for token in doc)
    has_negative = any(token.text in negative_words for token in doc)
    
    # 检查转折词
    contrast_words = ['but', 'however', 'although', 'yet']
    has_contrast = any(token.text in contrast_words for token in doc)
    
    # 检查夸张标点
    has_exaggeration = re.search(r'!{2,}', text) is not None
    
    # 简单规则: 同时包含正负面词且有转折或夸张
    if (has_positive and has_negative) and (has_contrast or has_exaggeration):
        return True
    return False

# 应用讽刺检测
sample_df['is_sarcastic'] = sample_df['text'].apply(detect_sarcasm)
print(f"Detected {sample_df['is_sarcastic'].sum()} potentially sarcastic comments")
```

### 6.2 领域自适应

```python
# 领域特定情感词典
domain_lexicons = {
    'TECH': {
        'positive': ['innovative', 'sleek', 'powerful', 'intuitive', 'responsive'],
        'negative': ['buggy', 'laggy', 'fragile', 'overpriced', 'outdated']
    },
    'POLITICS': {
        'positive': ['strong', 'effective', 'unifying', 'transparent', 'ethical'],
        'negative': ['corrupt', 'divisive', 'ineffective', 'scandal', 'controversial']
    }
}

def domain_aware_sentiment(text, category):
    # 使用通用分析器
    general_score = analyzer.polarity_scores(text)['compound']
    
    # 检查是否有领域特定词汇
    if category in domain_lexicons:
        domain_pos = domain_lexicons[category]['positive']
        domain_neg = domain_lexicons[category]['negative']
        
        pos_count = sum(1 for word in domain_pos if word in text.lower())
        neg_count = sum(1 for word in domain_neg if word in text.lower())
        
        # 调整分数
        adjusted_score = general_score + (pos_count * 0.1) - (neg_count * 0.1)
        return adjusted_score
    return general_score

# 应用领域感知情感分析
sample_df['domain_sentiment_score'] = sample_df.apply(
    lambda row: domain_aware_sentiment(row['text'], row['category']), axis=1
)
```

## 总结

这个实现涵盖了课程中提到的情感分析所有关键环节：
1. 完成了从实体提取、方面提取到情感分类的完整流程
2. 实现了规则驱动、统计分类和基于方面的三种情感分析方法
3. 针对商业应用场景开发了消费者洞察、竞争分析和舆情监控功能
4. 尝试解决课程提到的技术挑战(讽刺检测和领域适应)

代码可以直接应用于HuffPost新闻数据集，分析新闻中的情感倾向、被评价实体以及特定方面的情感态度。
